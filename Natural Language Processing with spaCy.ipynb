{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "138ccd9e",
   "metadata": {},
   "source": [
    "# 1. Introduction to NLP and spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7092a4",
   "metadata": {},
   "source": [
    "## Natural Language Processing (NLP) basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c28803",
   "metadata": {},
   "source": [
    "### Doc container in spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482a83fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load en_core_web_sm and create an nlp object\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Create a Doc container for the text object\n",
    "doc = nlp(text)\n",
    "\n",
    "# Create a list containing the text of each token in the Doc container\n",
    "print([token.text for token in doc])\n",
    "\n",
    "# ['NLP', 'is', 'becoming', 'increasingly', 'popular', 'for', 'providing', 'business', 'solutions', '.']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990bf4a4",
   "metadata": {},
   "source": [
    "### Tokenization with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe731005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Doc container of the given text\n",
    "document = nlp(text)\n",
    "    \n",
    "# Store and review the token text values of tokens for the Doc container\n",
    "first_text_tokens = [token.text for token in document]\n",
    "print(\"First text tokens:\\n\", first_text_tokens, \"\\n\")\n",
    "\n",
    "\"\"\"\n",
    "First text tokens:\n",
    "['I', 'have', 'bought', 'several', 'of', 'the', 'Vitality', 'canned', 'dog', 'food', 'products', 'and', 'have', 'found', \n",
    "'them', 'all', 'to', 'be', 'of', 'good', 'quality', '.', 'The', 'product', 'looks', 'more', 'like', 'a', 'stew', 'than', \n",
    "'a', 'processed', 'meat', 'and', 'it', 'smells', 'better', '.', 'My', 'Labrador', 'is', 'finicky', 'and', 'she', 'appreciates',\n",
    "'this', 'product', 'better', 'than', ' ', 'most', '.']\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55cb56b",
   "metadata": {},
   "source": [
    "## spaCy basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd79160e",
   "metadata": {},
   "source": [
    "### Running a spaCy pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7337495a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load en_core_web_sm model as nlp\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Run an nlp model on each item of texts and append the Doc container to documents\n",
    "documents = []\n",
    "for text in texts:\n",
    "  documents.append(nlp(text))\n",
    "  \n",
    "# Print the token texts for each Doc container\n",
    "for doc in documents:\n",
    "  print([token.text for token in doc])\n",
    "\n",
    "\"\"\"\n",
    "['A', 'loaded', 'spaCy', 'model', 'can', 'be', 'used', 'to', 'compile', 'documents', 'list', '!']\n",
    "['Tokenization', 'is', 'the', 'first', 'step', 'in', 'any', 'spacy', 'pipeline', '.']\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a22ad5c",
   "metadata": {},
   "source": [
    "### Lemmatization with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc704755",
   "metadata": {},
   "outputs": [],
   "source": [
    "document = nlp(text)\n",
    "tokens = [token.text for token in document]\n",
    "\n",
    "# Append the lemma for all tokens in the document\n",
    "lemmas = [token.lemma_ for token in document]\n",
    "print(\"Lemmas:\\n\", lemmas, \"\\n\")\n",
    "\n",
    "# Print tokens and compare with lemmas list\n",
    "print(\"Tokens:\\n\", tokens)\n",
    "\n",
    "\"\"\"\n",
    "Lemmas:\n",
    "['I', 'have', 'buy', 'several', 'of', 'the', 'vitality', 'can', 'dog', 'food', 'product', 'and', 'have', 'find', 'they', \n",
    "'all', 'to', 'be', 'of', 'good', 'quality', '.', 'the', 'product', 'look', 'more', 'like', 'a', 'stew', 'than', 'a', \n",
    "'process', 'meat', 'and', 'it', 'smell', 'well', '.', 'my', 'Labrador', 'be', 'finicky', 'and', 'she', 'appreciate', \n",
    "'this', 'product', 'well', 'than', ' ', 'most', '.'] \n",
    "\n",
    "\n",
    "Tokens:\n",
    "\n",
    "['I', 'have', 'bought', 'several', 'of', 'the', 'Vitality', 'canned', 'dog', 'food', 'products', 'and', 'have', 'found', \n",
    "'them', 'all', 'to', 'be', 'of', 'good', 'quality', '.', 'The', 'product', 'looks', 'more', 'like', 'a', 'stew', 'than', \n",
    "'a', 'processed', 'meat', 'and', 'it', 'smells', 'better', '.', 'My', 'Labrador', 'is', 'finicky', 'and', 'she', 'appreciates',\n",
    "'this', 'product', 'better', 'than', ' ', 'most', '.']\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7916b8c9",
   "metadata": {},
   "source": [
    "### Sentence segmentation with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a427f5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating a documents list of all Doc containers\n",
    "documents = [nlp(text) for text in texts]\n",
    "\n",
    "# Iterate through documents and append sentences in each doc to the sentences list\n",
    "sentences = []\n",
    "for doc in documents:\n",
    "  sentences.append([s for s in doc.sents])\n",
    "  \n",
    "# Find number of sentences per each doc container\n",
    "print([len(s) for s in sentences])\n",
    "\n",
    "# [3, 3, 8, 3, 4, 5, 5, 5, 3, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff57d6c",
   "metadata": {},
   "source": [
    "## Linguistic features in spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f275a0",
   "metadata": {},
   "source": [
    "### POS tagging with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c7f50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile a list of all Doc containers of texts\n",
    "documents = [nlp(text) for text in texts]\n",
    "\n",
    "# Print token texts and POS tags for each Doc container\n",
    "for doc in documents:\n",
    "    for token in doc:\n",
    "        print(\"Text: \", token.text, \"| POS tag: \", token.pos_)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "\"\"\"\n",
    "Text:  What | POS tag:  PRON\n",
    "    Text:  is | POS tag:  AUX\n",
    "    Text:  the | POS tag:  DET\n",
    "    Text:  arrival | POS tag:  NOUN\n",
    "    Text:  time | POS tag:  NOUN\n",
    "    Text:  in | POS tag:  ADP\n",
    "    Text:  San | POS tag:  PROPN\n",
    "    Text:  francisco | POS tag:  PROPN\n",
    "    Text:  for | POS tag:  ADP\n",
    "    Text:  the | POS tag:  DET\n",
    "    Text:  7:55 | POS tag:  NUM\n",
    "    Text:  AM | POS tag:  NOUN\n",
    "    Text:  flight | POS tag:  NOUN\n",
    "    Text:  leaving | POS tag:  VERB\n",
    "    Text:  Washington | POS tag:  PROPN\n",
    "    Text:  ? | POS tag:  PUNCT\n",
    "    \n",
    "    \n",
    "    Text:  Cheapest | POS tag:  PROPN\n",
    "    Text:  airfare | POS tag:  ADJ\n",
    "    Text:  from | POS tag:  ADP\n",
    "    Text:  Tacoma | POS tag:  PROPN\n",
    "    Text:  to | POS tag:  ADP\n",
    "    Text:  Orlando | POS tag:  PROPN\n",
    "    Text:  is | POS tag:  AUX\n",
    "    Text:  650 | POS tag:  NUM\n",
    "    Text:  dollars | POS tag:  NOUN\n",
    "    Text:  . | POS tag:  PUNCT\n",
    "    \n",
    "    \n",
    "    Text:  Round | POS tag:  ADJ\n",
    "    Text:  trip | POS tag:  NOUN\n",
    "    Text:  fares | POS tag:  NOUN\n",
    "    Text:  from | POS tag:  ADP\n",
    "    Text:  Pittsburgh | POS tag:  PROPN\n",
    "    Text:  to | POS tag:  ADP\n",
    "    Text:  Philadelphia | POS tag:  PROPN\n",
    "    Text:  are | POS tag:  AUX\n",
    "    Text:  under | POS tag:  ADP\n",
    "    Text:  1000 | POS tag:  NUM\n",
    "    Text:  dollars | POS tag:  NOUN\n",
    "    Text:  ! | POS tag:  PUNCT\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210bc447",
   "metadata": {},
   "source": [
    "### NER with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb087ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile a list of all Doc containers of texts\n",
    "documents = [nlp(text) for text in texts]\n",
    "\n",
    "# Print the entity text and label for the entities in each document\n",
    "for doc in documents:\n",
    "    print([(ent.text, ent.label_) for ent in doc.ents])\n",
    "    \n",
    "# Print the 6th token's text and entity type of the second document\n",
    "print(\"\\nText:\", documents[1][5].text, \"| Entity type: \", documents[1][5].ent_type_)\n",
    "\n",
    "\"\"\"\n",
    "    [('Boston', 'GPE'), ('8:38 am', 'TIME'), ('Denver', 'GPE'), ('11:10 in the morning', 'TIME')]\n",
    "    [('Pittsburgh', 'GPE'), ('Baltimore', 'GPE'), ('Thursday', 'DATE'), ('morning', 'TIME')]\n",
    "    [('San francisco', 'GPE'), ('Washington', 'GPE')]\n",
    "    \n",
    "    Text: Pittsburgh | Entity type:  GPE\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a008de",
   "metadata": {},
   "source": [
    "### Text processing with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546145ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list to store sentences of each Doc container in documents\n",
    "sentences = [[sent for sent in doc.sents] for doc in documents]\n",
    "\n",
    "# Create a list to track number of sentences per Doc container in documents\n",
    "num_sentences = [len([sent for sent in doc.sents]) for doc in documents]\n",
    "print(\"Number of sentences in documents:\\n\", num_sentences, \"\\n\")\n",
    "\n",
    "# Record entities text and corresponding label of the third Doc container\n",
    "third_text_entities = [(ent.text, ent.label_) for ent in documents[2].ents]\n",
    "print(\"Third text entities:\\n\", third_text_entities, \"\\n\")\n",
    "\n",
    "# Record first ten tokens and corresponding POS tag for the third Doc container\n",
    "third_text_10_pos = [(token.text, token.pos_) for token in documents[2]][:10]\n",
    "print(\"First ten tokens of third text:\\n\", third_text_10_pos)\n",
    "\n",
    "\"\"\"\n",
    "    Number of sentences in documents:\n",
    "     [3, 3, 8, 3, 4]\n",
    "\n",
    "\n",
    "    Number of sentences in documents:\n",
    "     [3, 3, 8, 3, 4] \n",
    "     \n",
    "    \n",
    "    Third text entities:\n",
    "     [('around a few centuries', 'DATE'), ('gelatin', 'ORG'), (\"C.S. Lewis'\", 'ORG'), ('The Lion, The Witch', 'WORK_OF_ART'), \n",
    "     ('Edmund', 'WORK_OF_ART'), ('Witch', 'ORG')] \n",
    "    \n",
    "    \n",
    "    First ten tokens of third text:\n",
    "     [('This', 'DET'), ('is', 'AUX'), ('a', 'DET'), ('confection', 'NOUN'), ('that', 'DET'), ('has', 'AUX'), ('been', 'VERB'),\n",
    "     ('around', 'ADV'), ('a', 'DET'), ('few', 'ADJ')]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0459fc1",
   "metadata": {},
   "source": [
    "# 2. spaCy Linguistic Annotations and Word Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0722f22",
   "metadata": {},
   "source": [
    "## Linguistic features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5599a6c",
   "metadata": {},
   "source": [
    "### Word-sense disambiguation with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e35e67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\"This device is used to jam the signal.\",\n",
    "         \"I am stuck in a traffic jam\"]\n",
    "\n",
    "# Create a list of Doc containers in the texts list\n",
    "documents = [nlp(t) for t in texts]\n",
    "\n",
    "# Print a token's text and POS tag if the word jam is in the token's text\n",
    "for i, doc in enumerate(documents):\n",
    "    print(f\"Sentence {i+1}: \", [(token.text, token.pos_) for token in doc if \"jam\" in token.text], \"\\n\")\n",
    "    \n",
    "\"\"\"\n",
    "    Sentence 1:  [('jam', 'VERB')] \n",
    "    \n",
    "    Sentence 2:  [('jam', 'NOUN')] \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2505ee6",
   "metadata": {},
   "source": [
    "### Dependency parsing with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907e6ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of Doc containts of texts list\n",
    "documents = [nlp(t) for t in texts]\n",
    "\n",
    "# Print each token's text, dependency label and its explanation\n",
    "for doc in documents:\n",
    "    print([(token.text, token.dep_, spacy.explain(token.dep_)) for token in doc], \"\\n\")\n",
    "    \n",
    "\"\"\"\n",
    "[('I', 'nsubj', 'nominal subject'), ('want', 'ROOT', None), ('to', 'aux', 'auxiliary'), \n",
    "('fly', 'xcomp', 'open clausal complement'), ('from', 'prep', 'prepositional modifier'), \n",
    "('Boston', 'pobj', 'object of preposition'), ('at', 'prep', 'prepositional modifier'), ('8:38', 'nummod', 'numeric modifier'), \n",
    "('am', 'pobj', 'object of preposition'), ('and', 'cc', 'coordinating conjunction'), ('arrive', 'conj', 'conjunct'), \n",
    "('in', 'prep', 'prepositional modifier'), ('Denver', 'pobj', 'object of preposition'), ('at', 'prep', 'prepositional modifier'),\n",
    "('11:10', 'pobj', 'object of preposition'), ('in', 'prep', 'prepositional modifier'), ('the', 'det', 'determiner'), \n",
    "('morning', 'pobj', 'object of preposition')] \n",
    "\n",
    "    \n",
    "[('What', 'det', 'determiner'), ('flights', 'nsubj', 'nominal subject'), ('are', 'ROOT', None), ('available', 'acomp', \n",
    "'adjectival complement'), ('from', 'prep', 'prepositional modifier'), ('Pittsburgh', 'pobj', 'object of preposition'), \n",
    "('to', 'prep', 'prepositional modifier'), ('Baltimore', 'pobj', 'object of preposition'), ('on', 'prep', \n",
    "'prepositional modifier'), ('Thursday', 'compound', 'compound'), ('morning', 'pobj', 'object of preposition'), \n",
    "('?', 'punct', 'punctuation')] \n",
    "    \n",
    "    \n",
    "[('What', 'attr', 'attribute'), ('is', 'ROOT', None), ('the', 'det', 'determiner'), ('arrival', 'compound', 'compound'), \n",
    "('time', 'nsubj', 'nominal subject'), ('in', 'prep', 'prepositional modifier'), ('San', 'compound', 'compound'), \n",
    "('francisco', 'pobj', 'object of preposition'), ('for', 'prep', 'prepositional modifier'), ('the', 'det', 'determiner'), \n",
    "('7:55', 'nummod', 'numeric modifier'), ('AM', 'compound', 'compound'), ('flight', 'pobj', 'object of preposition'), \n",
    "('leaving', 'acl', 'clausal modifier of noun (adjectival clause)'), ('Washington', 'dobj', 'direct object'), \n",
    "('?', 'punct', 'punctuation')] \n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16e6b99",
   "metadata": {},
   "source": [
    "## Introduction to word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead88c8c",
   "metadata": {},
   "source": [
    "### spaCy vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d7f4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the en_core_web_md model\n",
    "md_nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Print the number of words in the model's vocabulary\n",
    "print(\"Number of words: \", md_nlp.meta[\"vectors\"][\"vectors\"], \"\\n\")\n",
    "\n",
    "# Print the dimensions of word vectors in en_core_web_md model\n",
    "print(\"Dimension of word vectors: \", md_nlp.meta[\"vectors\"][\"width\"])\n",
    "\n",
    "\"\"\"\n",
    "    Number of words:  20000 \n",
    "    \n",
    "    Dimension of word vectors:  300\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6d1701",
   "metadata": {},
   "source": [
    "### Word vectors in spaCy vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed90550",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"like\", \"love\"]\n",
    "\n",
    "# IDs of all the given words\n",
    "ids = [nlp.vocab.strings[w] for w in words]\n",
    "\n",
    "# Store the first ten elements of the word vectors for each word\n",
    "word_vectors = [nlp.vocab.vectors[i][:10] for i in ids]\n",
    "\n",
    "# Print the first ten elements of the first word vector\n",
    "print(word_vectors[0])\n",
    "\n",
    "\"\"\"\n",
    "    [-0.18417   0.055115 -0.36953  -0.20895   0.25672   0.30142   0.16299\n",
    "     -0.16437  -0.070268  2.1638  ]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf30bd4",
   "metadata": {},
   "source": [
    "## Word vectors and spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab154b9",
   "metadata": {},
   "source": [
    "### Word vectors projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbc4bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"tiger\", \"bird\"]\n",
    "\n",
    "# Extract word IDs of given words\n",
    "word_ids = [nlp.vocab.strings[w] for w in words]\n",
    "\n",
    "# Extract word vectors and stack the first five elements vertically\n",
    "word_vectors = np.vstack([nlp.vocab.vectors[i][:5] for i in word_ids])\n",
    "\n",
    "# Calculate the transformed word vectors using the pca object\n",
    "pca = PCA(n_components=2)\n",
    "word_vectors_transformed = pca.fit_transform(word_vectors)\n",
    "\n",
    "# Print the first component of the transformed word vectors\n",
    "print(word_vectors_transformed[:, 0])\n",
    "\n",
    "\"\"\"\n",
    "[ 0.5182773 -0.5182773]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baea1a3c",
   "metadata": {},
   "source": [
    "### Similar words in a vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4319d788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the most similar word to the word computer\n",
    "most_similar_words = nlp.vocab.vectors.most_similar(np.asarray([word_vector]), n = 1)\n",
    "\n",
    "# Find the list of similar words given the word IDs\n",
    "words = [nlp.vocab.strings[w] for w in most_similar_words[0][0]]\n",
    "print(words)\n",
    "\n",
    "# ['computer-related']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d79f1c",
   "metadata": {},
   "source": [
    "### Measuring semantic similarity with spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653883a0",
   "metadata": {},
   "source": [
    "### Doc similarity with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8007b2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a documents list containing Doc containers\n",
    "documents = [nlp(t) for t in texts]\n",
    "\n",
    "# Create a Doc container of the category\n",
    "category = \"canned dog food\"\n",
    "category_document = nlp(category)\n",
    "\n",
    "# Print similarity scores of each Doc container and the category_document\n",
    "for i, doc in enumerate(documents):\n",
    "  print(f\"Semantic similarity with document {i+1}:\", round(doc.similarity(category_document), 3))\n",
    "\n",
    "\"\"\"\n",
    "    Semantic similarity with document 1: 0.84\n",
    "    Semantic similarity with document 2: 0.561\n",
    "    Semantic similarity with document 3: 0.577\n",
    "    Semantic similarity with document 4: 0.583\n",
    "    Semantic similarity with document 5: 0.526\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57f602b",
   "metadata": {},
   "source": [
    "### Span similarity with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0a5474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Doc container for the category\n",
    "category = \"canned dog food\"\n",
    "category_document = nlp(category)\n",
    "\n",
    "# Print similarity score of a given Span and category_document\n",
    "document_span = document[0:3]\n",
    "print(f\"Semantic similarity with\", document_span.text, \":\", round(document_span.similarity(category_document), 3))\n",
    "\n",
    "# Semantic similarity with canned food products : 0.866"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62827e8",
   "metadata": {},
   "source": [
    "### Semantic similarity for categorizing text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69d398f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populate Doc containers for the word \"sauce\" and for \"texts\" string \n",
    "key = nlp(\"sauce\")\n",
    "sentences = nlp(texts)\n",
    "\n",
    "# Calculate similarity score of each sentence and a Doc container for the word sauce\n",
    "semantic_scores = []\n",
    "for sent in sentences.sents:\n",
    "    semantic_scores.append({\"score\": round(sent.similarity(key), 2)})\n",
    "print(semantic_scores)\n",
    "\n",
    "\"\"\"\n",
    "[{'score': 0.65}, {'score': 0.35}]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69f2035",
   "metadata": {},
   "source": [
    "# 3. Data Analysis with spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb2189d",
   "metadata": {},
   "source": [
    "## spaCy pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b6acc5",
   "metadata": {},
   "source": [
    "### Adding pipes in spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a193e43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a blank spaCy English model and add a sentencizer component\n",
    "nlp = spacy.blank(\"en\")\n",
    "nlp.add_pipe(\"sentencizer\")\n",
    "\n",
    "# Create Doc containers, store sentences and print its number of sentences\n",
    "doc = nlp(texts)\n",
    "sentences = [s for s in doc.sents]\n",
    "print(\"Number of sentences: \", len(sentences), \"\\n\")\n",
    "\n",
    "# Print the list of tokens in the second sentence\n",
    "print(\"Second sentence tokens: \", [token for token in sentences[1]])\n",
    "\n",
    "\"\"\"\n",
    "Number of sentences:  19 \n",
    "    \n",
    "Second sentence tokens:  [The, product, looks, more, like, a, stew, than, a, processed, meat, and, it, smells, better, .]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171dbf01",
   "metadata": {},
   "source": [
    "### Analyzing pipelines in spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b74472f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a blank spaCy English model\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Add tagger and entity_linker pipeline components\n",
    "nlp.add_pipe(\"tagger\")\n",
    "nlp.add_pipe(\"entity_linker\")\n",
    "\n",
    "# Analyze the pipeline\n",
    "analysis = nlp.analyze_pipes(pretty=True)\n",
    "\n",
    "\"\"\"\n",
    "    \u001b[1m\n",
    "    ============================= Pipeline Overview =============================\u001b[0m\n",
    "    \n",
    "    #   Component       Assigns           Requires         Scores        Retokenizes\n",
    "    -   -------------   ---------------   --------------   -----------   -----------\n",
    "    0   tagger          token.tag                          tag_acc       False      \n",
    "                                                                                    \n",
    "    1   entity_linker   token.ent_kb_id   doc.ents         nel_micro_f   False      \n",
    "                                          doc.sents        nel_micro_r              \n",
    "                                          token.ent_iob    nel_micro_p              \n",
    "                                          token.ent_type                            \n",
    "    \n",
    "    \u001b[1m\n",
    "    ================================ Problems (4) ================================\u001b[0m\n",
    "    \u001b[38;5;3mâš  'entity_linker' requirements not met: doc.ents, doc.sents,\n",
    "    token.ent_iob, token.ent_type\u001b[0m\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6016bd1",
   "metadata": {},
   "source": [
    "## spaCy EntityRuler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8cab39",
   "metadata": {},
   "source": [
    "### EntityRuler with blank spaCy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c30405",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"en\")\n",
    "patterns = [{\"label\": \"ORG\", \"pattern\": [{\"LOWER\": \"openai\"}]},\n",
    "            {\"label\": \"ORG\", \"pattern\": [{\"LOWER\": \"microsoft\"}]}]\n",
    "text = \"OpenAI has joined forces with Microsoft.\"\n",
    "\n",
    "# Add EntityRuler component to the model\n",
    "entity_ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "\n",
    "# Add given patterns to the EntityRuler component\n",
    "entity_ruler.add_patterns(patterns)\n",
    "\n",
    "# Run the model on a given text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Print entities text and type for all entities in the Doc container\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])\n",
    "\n",
    "# [('OpenAI', 'ORG'), ('Microsoft', 'ORG')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ea2abd",
   "metadata": {},
   "source": [
    "### EntityRuler for NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35237631",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text = \"New York Group was built in 1987.\"\n",
    "\n",
    "# Add an EntityRuler to the nlp before NER component\n",
    "ruler = nlp.add_pipe(\"entity_ruler\", before='ner')\n",
    "\n",
    "# Define a pattern to classify lower cased new york group as ORG\n",
    "patterns = [{\"label\": \"ORG\", \"pattern\": [{\"lower\": \"new york group\"}]}]\n",
    "\n",
    "# Add the patterns to the EntityRuler component\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "# Run the model and print entities text and type for all the entities\n",
    "doc = nlp(text)\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])\n",
    "\n",
    "# [('New York Group', 'ORG'), ('1987', 'DATE')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2174ee",
   "metadata": {},
   "source": [
    "### EntityRuler with multi-patterns in spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a02ba31",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Print a list of tuples of entities text and types in the example_text\n",
    "print(\"Before EntityRuler: \", [(ent.text, ent.label_) for ent in nlp(example_text).ents], \"\\n\")\n",
    "\n",
    "# Define pattern to add a label PERSON for lower cased sisters and brother entities\n",
    "patterns = [{\"label\": \"PERSON\", \"pattern\": [{\"lower\": \"sisters\"}]},\n",
    "            {\"label\": \"PERSON\", \"pattern\": [{\"lower\": \"brother\"}]}]\n",
    "\n",
    "# Add an EntityRuler component and add the patterns to the ruler\n",
    "ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "# Print a list of tuples of entities text and types\n",
    "print(\"After EntityRuler: \", [(ent.text, ent.label_) for ent in nlp(example_text).ents])\n",
    "\n",
    "\"\"\"\n",
    "Before EntityRuler:  [('Filberts', 'ORG'), ('Edmund', 'PERSON')] \n",
    "    \n",
    "After EntityRuler:  [('Filberts', 'ORG'), ('Edmund', 'PERSON'), ('Brother', 'PERSON'), ('Sisters', 'PERSON')]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2286b8e6",
   "metadata": {},
   "source": [
    "## RegEx with spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195d28e4",
   "metadata": {},
   "source": [
    "### RegEx in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4009a926",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Our phone number is (425)-123-4567.\"\n",
    "\n",
    "# Define a pattern to match phone numbers\n",
    "pattern = r\"\\((\\d){3}\\)-(\\d){3}-(\\d){4}\"\n",
    "\n",
    "# Find all the matching patterns in the text\n",
    "phones = re.finditer(pattern, text)\n",
    "\n",
    "# Print start and end characters and matching section of the text\n",
    "for match in phones:\n",
    "    start_char = match.start()\n",
    "    end_char = match.end()\n",
    "    print(\"Start character: \", start_char, \"| End character: \", end_char, \"| Matching text: \", text[start_char:end_char])\n",
    "    \n",
    "\"\"\"\n",
    "Start character:  20 | End character:  34 | Matching text:  (425)-123-4567\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12755b8b",
   "metadata": {},
   "source": [
    "### RegEx with EntityRuler in spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d6a7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Our phone number is 4251234567.\"\n",
    "\n",
    "# Define a pattern to match phone numbers\n",
    "patterns = [{\"label\": \"PHONE_NUMBERS\", \"pattern\": [{\"TEXT\": {\"REGEX\": \"(\\d){10}\"}}]}]\n",
    "\n",
    "# Load a blank model and add an EntityRuler\n",
    "nlp = spacy.blank(\"en\")\n",
    "ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "\n",
    "# Add the compiled patterns to the EntityRuler\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "# Print the tuple of entities texts and types for the given text\n",
    "doc = nlp(text)\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])\n",
    "\n",
    "# [('4251234567', 'PHONE_NUMBERS')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7f0e7d",
   "metadata": {},
   "source": [
    "## spaCy Matcher and PhraseMatcher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a368ce27",
   "metadata": {},
   "source": [
    "### Matching a single term in spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b92022",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(example_text)\n",
    "\n",
    "# Initialize a Matcher object\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Define a pattern to match lower cased word witch\n",
    "pattern = [{\"lower\": \"witch\"}]\n",
    "\n",
    "# Add the pattern to matcher object and find matches\n",
    "matcher.add(\"CustomMatcher\", [pattern])\n",
    "matches = matcher(doc)\n",
    "\n",
    "# Print start and end token indices and span of the matched text\n",
    "for match_id, start, end in matches:\n",
    "    print(\"Start token: \", start, \" | End token: \", end, \"| Matched text: \", doc[start:end].text)\n",
    "    \n",
    "\"\"\"\n",
    "    Start token:  24  | End token:  25 | Matched text:  Witch\n",
    "    Start token:  47  | End token:  48 | Matched text:  Witch\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b491cc8",
   "metadata": {},
   "source": [
    "### PhraseMatcher in spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5e448d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"There are only a few acceptable IP addresse: (1) 127.100.0.1, (2) 123.4.1.0.\"\n",
    "terms = [\"110.0.0.0\", \"101.243.0.0\"]\n",
    "\n",
    "# Initialize a PhraseMatcher class to match to shapes of given terms\n",
    "matcher = PhraseMatcher(nlp.vocab, attr = \"SHAPE\")\n",
    "\n",
    "# Create patterns to add to the PhraseMatcher object\n",
    "patterns = [nlp.make_doc(term) for term in terms]\n",
    "matcher.add(\"IPAddresses\", patterns)\n",
    "\n",
    "# Find matches to the given patterns and print start and end characters and matches texts\n",
    "doc = nlp(text)\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    print(\"Start token: \", start, \" | End token: \", end, \"| Matched text: \", doc[start:end].text)\n",
    "    \n",
    "\"\"\"\n",
    "    Start token:  12  | End token:  13 | Matched text:  127.100.0.1\n",
    "    Start token:  17  | End token:  18 | Matched text:  123.4.1.0\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ad1a36",
   "metadata": {},
   "source": [
    "### Matching with extended syntax in spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8beacca",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(example_text)\n",
    "\n",
    "# Define a matcher object\n",
    "matcher = Matcher(nlp.vocab)\n",
    "# Define a pattern to match tiny squares and tiny mouthful\n",
    "pattern = [{\"lower\": \"tiny\"}, {\"lower\": {\"IN\": [\"squares\", \"mouthful\"]}}]\n",
    "\n",
    "# Add the pattern to matcher object and find matches\n",
    "matcher.add(\"CustomMatcher\", [pattern])\n",
    "matches = matcher(doc)\n",
    "\n",
    "# Print out start and end token indices and the matched text span per match\n",
    "for match_id, start, end in matches:\n",
    "    print(\"Start token: \", start, \" | End token: \", end, \"| Matched text: \", doc[start:end].text)\n",
    "    \n",
    "\"\"\"\n",
    "    Start token:  4  | End token:  6 | Matched text:  tiny squares\n",
    "    Start token:  19  | End token:  21 | Matched text:  tiny mouthful\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc5765f",
   "metadata": {},
   "source": [
    "# 4. Customizing spaCy Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f86a2d",
   "metadata": {},
   "source": [
    "## Customizing spaCy models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b92f69",
   "metadata": {},
   "source": [
    "### Model performance on your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e21a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append a tuple of (entities text, entities label) if Jumbo is in the entity\n",
    "target_entities = []\n",
    "for doc in documents:\n",
    "  target_entities.extend([(ent.text, ent.label_) for ent in doc.ents if \"Jumbo\" in ent.text])\n",
    "print(target_entities)\n",
    "\n",
    "# Append True to the correct_labels list if the entity label is `PRODUCT`\n",
    "correct_labels = []\n",
    "for ent in target_entities:\n",
    "  if ent[1] == \"PRODUCT\":\n",
    "    correct_labels.append(True)\n",
    "  else:\n",
    "    correct_labels.append(False)\n",
    "print(correct_labels)\n",
    "\n",
    "\"\"\"\n",
    "    [('Jumbo', 'PERSON'), ('Jumbo', 'PERSON')]\n",
    "    [False, False]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a9f49f",
   "metadata": {},
   "source": [
    "## spaCy training data format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1eb050",
   "metadata": {},
   "source": [
    "### Annotation and preparing training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82735502",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"A patient with chest pain had hyperthyroidism.\"\n",
    "entity_1 = \"chest pain\"\n",
    "entity_2 = \"hyperthyroidism\"\n",
    "\n",
    "# Store annotated data information in the correct format\n",
    "annotated_data = {\"sentence\": text, \"entities\": [{\"label\": \"SYMPTOM\", \"value\": entity_1}, {\"label\": \"DISEASE\", \"value\": entity_2}]}\n",
    "\n",
    "# Extract start and end characters of each entity\n",
    "entity_1_start_char = text.find(entity_1)\n",
    "entity_1_end_char = entity_1_start_char + len(entity_1)\n",
    "entity_2_start_char = text.find(entity_2)\n",
    "entity_2_end_char = entity_2_start_char + len(entity_2)\n",
    "\n",
    "# Store the same input information in the proper format for training\n",
    "training_data = [(text, {\"entities\": [(entity_1_start_char, entity_1_end_char, \"SYMPTOM\"), \n",
    "                                      (entity_2_start_char, entity_2_end_char, \"DISEASE\")]})]\n",
    "print(training_data)\n",
    "\n",
    "\n",
    "\"\"\"  \n",
    "[('A patient with chest pain had hyperthyroidism.', {'entities': [(15, 25, 'SYMPTOM'), (30, 45, 'DISEASE')]})]\n",
    "\n",
    "\"\"\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57eb62fb",
   "metadata": {},
   "source": [
    "### Compatible training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc2df59",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = 'A patient with chest pain had hyperthyroidism.'\n",
    "training_data = [(example_text, {'entities': [(15, 25, 'SYMPTOM'), (30, 45, 'DISEASE')]})]\n",
    "\n",
    "all_examples = []\n",
    "# Iterate through text and annotations and convert text to a Doc container\n",
    "for text, annotations in training_data:\n",
    "  doc = nlp(text)\n",
    "  \n",
    "  # Create an Example object from the doc contianer and annotations\n",
    "  example_sentence = Example.from_dict(doc, annotations)\n",
    "  print(example_sentence.to_dict(), \"\\n\")\n",
    "  \n",
    "  # Append the Example object to the list of all examples\n",
    "  all_examples.append(example_sentence)\n",
    "  \n",
    "print(\"Number of formatted training data: \", len(all_examples))\n",
    "\n",
    "\"\"\"\n",
    "{'doc_annotation': {'cats': {}, 'entities': ['O', 'O', 'O', 'B-SYMPTOM', 'L-SYMPTOM', 'O', 'U-DISEASE', 'O'], 'links': {}}, \n",
    "'token_annotation': {'ORTH': ['A', 'patient', 'with', 'chest', 'pain', 'had', 'hyperthyroidism', '.'], \n",
    "'SPACY': [True, True, True, True, True, True, False, False], 'TAG': ['', '', '', '', '', '', '', ''], \n",
    "'LEMMA': ['', '', '', '', '', '', '', ''], 'POS': ['', '', '', '', '', '', '', ''], 'MORPH': ['', '', '', '', '', '', '', ''],\n",
    "'HEAD': [0, 1, 2, 3, 4, 5, 6, 7], 'DEP': ['', '', '', '', '', '', '', ''], 'SENT_START': [1, 0, 0, 0, 0, 0, 0, 0]}} \n",
    "    \n",
    " Number of formatted training data:  1\n",
    " \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4474fd43",
   "metadata": {},
   "source": [
    "## Training with spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94cdf35",
   "metadata": {},
   "source": [
    "### Training preparation steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9578c002",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Disable all pipeline components of  except `ner`\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "nlp.disable_pipes(*other_pipes)\n",
    "\n",
    "# Convert a text and its annotations to the correct format usable for training\n",
    "doc = nlp.make_doc(text)\n",
    "example = Example.from_dict(doc, annotations)\n",
    "print(\"Example object for training: \\n\", example.to_dict())\n",
    "\n",
    "\"\"\"\n",
    "Example object for training: \n",
    "\n",
    "{'doc_annotation': {'cats': {}, 'entities': ['O', 'O', 'O', 'O', 'O', 'U-GPE', 'O'], 'links': {}}, \n",
    "'token_annotation': {'ORTH': ['I', 'will', 'visit', 'you', 'in', 'Austin', '.'], \n",
    "'SPACY': [True, True, True, True, True, False, False], 'TAG': ['', '', '', '', '', '', ''], \n",
    "'LEMMA': ['', '', '', '', '', '', ''], 'POS': ['', '', '', '', '', '', ''], 'MORPH': ['', '', '', '', '', '', ''], \n",
    "'HEAD': [0, 1, 2, 3, 4, 5, 6], 'DEP': ['', '', '', '', '', '', ''], 'SENT_START': [1, 0, 0, 0, 0, 0, 0]}}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6431459d",
   "metadata": {},
   "source": [
    "### Train an existing NER model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e53c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "print(\"Before training: \", [(ent.text, ent.label_)for ent in nlp(test).ents])\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "nlp.disable_pipes(*other_pipes)\n",
    "optimizer = nlp.create_optimizer()\n",
    "\n",
    "# Shuffle training data and the dataset using random package per epoch\n",
    "for i in range(epochs):\n",
    "  random.shuffle(training_data)\n",
    "  for text, annotations in training_data:\n",
    "    doc = nlp.make_doc(text)\n",
    "    # Update nlp model after setting sgd argument to optimizer\n",
    "    example = Example.from_dict(doc, annotations)\n",
    "    nlp.update([example], sgd = optimizer)\n",
    "print(\"After training: \", [(ent.text, ent.label_)for ent in nlp(test).ents])\n",
    "\n",
    "\"\"\"\n",
    "    Before training:  [('Sam', 'PERSON')]\n",
    "    After training:  [('Sam', 'PERSON'), ('house', 'GPE')]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862653db",
   "metadata": {},
   "source": [
    "### Training a spaCy model from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61203b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a blank English model, add NER component, add given labels to the ner pipeline\n",
    "nlp = spacy.blank(\"en\")\n",
    "ner = nlp.add_pipe(\"ner\")\n",
    "for ent in labels:\n",
    "    ner.add_label(ent)\n",
    "\n",
    "# Disable other pipeline components, complete training loop and run training loop\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "nlp.disable_pipes(*other_pipes)\n",
    "losses = {}\n",
    "optimizer = nlp.begin_training()\n",
    "for text, annotation in training_data:\n",
    "    doc = nlp.make_doc(text)\n",
    "    example = Example.from_dict(doc, annotation)\n",
    "    nlp.update([example], sgd=optimizer, losses=losses)\n",
    "    print(losses)\n",
    "    \n",
    "\"\"\"\n",
    "    {'ner': 98.42856240272522}\n",
    "    {'ner': 189.34197914600372}\n",
    "    {'ner': 310.98384952545166}\n",
    "    {'ner': 377.18853384256363}\n",
    "    {'ner': 578.3004557490349}\n",
    "    {'ner': 608.2531691193581}\n",
    "    {'ner': 673.164011657238}\n",
    "    {'ner': 723.6257713735104}\n",
    "    {'ner': 746.0405215527862}\n",
    "    {'ner': 756.4262755732052}\n",
    "    {'ner': 790.0200939634815}\n",
    "    {'ner': 795.990012896822}\n",
    "    {'ner': 796.0241677922168}\n",
    "    {'ner': 798.0212131336592}\n",
    "    {'ner': 813.9837758949773}\n",
    "    {'ner': 817.9703837852048}\n",
    "    {'ner': 829.9056551901582}\n",
    "    {'ner': 845.8729668580933}\n",
    "    {'ner': 853.8427640225843}\n",
    "    {'ner': 863.77342558173}\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
