{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e12741ed",
   "metadata": {},
   "source": [
    "# Introduction to Natural Language Processing in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b89d79e",
   "metadata": {},
   "source": [
    "# 1. Regular expressions & word tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9751433b",
   "metadata": {},
   "source": [
    "## Introduction to regular expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9321a802",
   "metadata": {},
   "source": [
    "### Practicing regular expressions: re.split() and re.findall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2660e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a pattern to match sentence endings: sentence_endings\n",
    "sentence_endings = r\"[.?!]\"\n",
    "\n",
    "# Split my_string on sentence endings and print the result\n",
    "print(re.split(sentence_endings, my_string))\n",
    "\n",
    "# Find all capitalized words in my_string and print the result\n",
    "capitalized_words = r\"[A-Z]\\w+\"\n",
    "print(re.findall(capitalized_words, my_string))\n",
    "\n",
    "# Split my_string on spaces and print the result\n",
    "spaces = r\"\\s+\"\n",
    "print(re.split(spaces, my_string))\n",
    "\n",
    "\"\"\"\n",
    "    [\"Let's write RegEx\", \"  Won't that be fun\", '  I sure think so', '  Can you find 4 sentences', '  Or perhaps, all 19 words', '']\n",
    "    ['Let', 'RegEx', 'Won', 'Can', 'Or']\n",
    "    [\"Let's\", 'write', 'RegEx!', \"Won't\", 'that', 'be', 'fun?', 'I', 'sure', 'think', 'so.', 'Can', 'you', 'find', '4', 'sentences?', 'Or', 'perhaps,', 'all', '19', 'words?']\n",
    "    ['4', '19']\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b7c107",
   "metadata": {},
   "source": [
    "## Introduction to tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623a1259",
   "metadata": {},
   "source": [
    "### Word tokenization with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92acef16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Split scene_one into sentences: sentences\n",
    "sentences = sent_tokenize(scene_one)\n",
    "\n",
    "# Use word_tokenize to tokenize the fourth sentence: tokenized_sent\n",
    "tokenized_sent = word_tokenize(sentences[3])\n",
    "\n",
    "# Make a set of unique tokens in the entire scene: unique_tokens\n",
    "unique_tokens = set(word_tokenize(scene_one))\n",
    "\n",
    "# Print the unique tokens result\n",
    "print(unique_tokens)\n",
    "\n",
    "\"\"\"\n",
    " {':', 'you', 'England', 'land', 'other', 'No', 'house', 'the', \"n't\", 'feathers', \"'s\", 'got', 'these', 'plover', 'mean', \n",
    " 'European', 'guiding', 'A', 'Uther', 'breadth', 'You', 'is', 'get', 'fly', 'interested', 'Well', 'coconuts', 'using', \n",
    " 'carrying', 'found', 'husk', 'why', '[', 'martin', 'servant', 'are', '2', 'point', 'wind', 'ratios', 'defeator', 'Mercea', \n",
    " 'two', 'grips', 'horse', 'Wait', 'of', 'African', 'creeper', 'and', 'zone', 'matter', 'empty', 'That', 'wings', 'must', \n",
    " 'under', 'It', 'right', 'me', 'I', 'my', 'warmer', 'Halt', 'Whoa', 'sun', 'carry', 'second', 'but', '#', 'coconut', 'weight',\n",
    " 'Who', 'maintain', 'ask', 'they', 'have', 'be', 'together', 'But', 'Not', 'strand', 'knights', 'from', 'times', 'course', \n",
    " 'order', 'goes', 'What', 'swallow', 'in', 'a', 'ounce', 'velocity', 'am', '...', 'Ridden', 'our', 'just', 'son', 'Please', \n",
    " 'with', 'one', 'south', 'tell', '!', 'They', 'SCENE', 'do', 'yet', 'migrate', 'question', '--', 'to', 'bird', 'not', \n",
    " 'suggesting', 'that', 'agree', 'sovereign', ']', 'beat', 'where', 'Are', 'The', 'needs', '1', 'join', 'an', 'or', 'five', \n",
    " 'line', 'yeah', 'strangers', 'KING', 'Yes', \"'d\", 'could', 'there', 'through', 'does', 'anyway', 'since', 'Found', 'lord', \n",
    " 'will', \"'\", 'Oh', 'then', 'Arthur', 'ARTHUR', 'Listen', 'go', 'winter', 'court', 'its', 'So', 'carried', 'non-migratory', \n",
    " 'bring', 'every', 'Supposing', 'length', 'dorsal', 'he', 'Pull', 'held', 'at', 'trusty', 'clop', 'temperate', 'them', \n",
    " 'halves', 'Saxons', \"'ve\", 'In', 'it', 'Court', 'covered', '?', 'snows', '.', 'We', 'Pendragon', 'if', 'here', 'on', \n",
    " 'swallows', 'forty-three', 'who', 'tropical', 'Patsy', 'search', 'ridden', 'SOLDIER', 'simple', 'King', 'seek', 'Will',\n",
    " 'Camelot', 'grip', 'may', 'maybe', 'by', 'master', 'wants', 'air-speed', 'bangin', \"'em\", 'climes', 'all', 'use', 'castle',\n",
    " ',', 'Where', \"'m\", 'minute', 'speak', 'pound', 'your', 'Am', 'back', \"'re\", 'kingdom', 'Britons', 'this'}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9209f53e",
   "metadata": {},
   "source": [
    "### More regex with re.search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed747004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for the first occurrence of \"coconuts\" in scene_one: match\n",
    "match = re.search(\"coconuts\", scene_one)\n",
    "\n",
    "# Print the start and end indexes of match\n",
    "print(match.start(), match.end())\n",
    "\n",
    "# 580 588"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196442f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a regular expression to search for anything in square brackets: pattern1\n",
    "pattern1 = r\"\\[.*\\]\"\n",
    "\n",
    "# Use re.search to find the first text in square brackets\n",
    "print(re.search(pattern1, scene_one))\n",
    "\n",
    "#  <re.Match object; span=(9, 32), match='[wind] [clop clop clop]'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570083ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the script notation at the beginning of the fourth sentence and print it\n",
    "pattern2 = r\"[\\w\\s]+:\"\n",
    "print(re.match(pattern2, sentences[3]))\n",
    "\n",
    "# <re.Match object; span=(0, 7), match='ARTHUR:'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4802686",
   "metadata": {},
   "source": [
    "## Advanced tokenization with NLTK and regex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ca2bd7",
   "metadata": {},
   "source": [
    "### Regex with NLTK tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7433bb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db90469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "# Define a regex pattern to find hashtags: pattern1\n",
    "pattern1 = r\"#\\w+\"\n",
    "# Use the pattern on the first tweet in the tweets list\n",
    "hashtags = regexp_tokenize(tweets[0], pattern1)\n",
    "print(hashtags)\n",
    "\n",
    "# ['#nlp', '#python']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78bb8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "# Write a pattern that matches both mentions (@) and hashtags\n",
    "pattern2 = r\"([@#]\\w+)\"\n",
    "# Use the pattern on the last tweet in the tweets list\n",
    "mentions_hashtags = regexp_tokenize(tweets[-1], pattern2)\n",
    "print(mentions_hashtags)\n",
    "\n",
    "#  ['@datacamp', '#nlp', '#python']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95768a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "# Use the TweetTokenizer to tokenize all tweets into one list\n",
    "tknzr = TweetTokenizer()\n",
    "all_tokens = [tknzr.tokenize(t) for t in tweets]\n",
    "print(all_tokens)\n",
    "\n",
    "\"\"\" \n",
    "[['This', 'is', 'the', 'best', '#nlp', 'exercise', 'ive', 'found', 'online', '!', '#python'], \n",
    "['#NLP', 'is', 'super', 'fun', '!', '<3', '#learning'], ['Thanks', '@datacamp', ':)', '#nlp', '#python']] \n",
    "\"\"\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff99497d",
   "metadata": {},
   "source": [
    "### Non-ascii tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab3f383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and print all words in german_text\n",
    "all_words = word_tokenize(german_text)\n",
    "print(all_words)\n",
    "\n",
    "# Tokenize and print only capital words\n",
    "capital_words = r\"[A-Z√ú]\\w+\"\n",
    "print(regexp_tokenize(german_text, capital_words))\n",
    "\n",
    "# Tokenize and print only emoji\n",
    "emoji = \"['\\U0001F300-\\U0001F5FF'|'\\U0001F600-\\U0001F64F'|'\\U0001F680-\\U0001F6FF'|'\\u2600-\\u26FF\\u2700-\\u27BF']\"\n",
    "print(regexp_tokenize(german_text, emoji))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "['Wann', 'gehen', 'wir', 'Pizza', 'essen', '?', 'üçï', 'Und', 'f√§hrst', 'du', 'mit', '√úber', '?', 'üöï']\n",
    "['Wann', 'Pizza', 'Und', '√úber']\n",
    "['üçï', 'üöï']\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2a9c33",
   "metadata": {},
   "source": [
    "## Charting word length with NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107714ce",
   "metadata": {},
   "source": [
    "### Charting practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564fac8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the script into lines: lines\n",
    "lines = holy_grail.split('\\n')\n",
    "\n",
    "# Replace all script lines for speaker\n",
    "pattern = \"[A-Z]{2,}(\\s)?(#\\d)?([A-Z]{2,})?:\"\n",
    "lines = [re.sub(pattern, '', l) for l in lines]\n",
    "\n",
    "# Tokenize each line: tokenized_lines\n",
    "tokenized_lines = [regexp_tokenize(s, \"\\w+\") for s in lines]\n",
    "\n",
    "# Make a frequency list of lengths: line_num_words\n",
    "line_num_words = [len(t_line) for t_line in tokenized_lines]\n",
    "\n",
    "# Plot a histogram of the line lengths\n",
    "plt.hist(line_num_words)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81133158",
   "metadata": {},
   "source": [
    "# 2. Simple topic identification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9163ea",
   "metadata": {},
   "source": [
    "## Word counts with bag-of-words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d9d0d8",
   "metadata": {},
   "source": [
    "### Building a Counter with bag-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe44fa8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Counter\n",
    "from collections import Counter\n",
    "\n",
    "# Tokenize the article: tokens\n",
    "tokens = word_tokenize(article)\n",
    "\n",
    "# Convert the tokens into lowercase: lower_tokens\n",
    "lower_tokens = [t.lower() for t in tokens]\n",
    "\n",
    "# Create a Counter with the lowercase tokens: bow_simple\n",
    "bow_simple = Counter(lower_tokens)\n",
    "\n",
    "# Print the 10 most common tokens\n",
    "print(bow_simple.most_common(10))\n",
    "\n",
    "\"\"\"\n",
    "[(',', 151), ('the', 150), ('.', 89), ('of', 81), (\"''\", 66), ('to', 63), ('a', 60), ('``', 47), ('in', 44), ('and', 41)]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf948dd",
   "metadata": {},
   "source": [
    "## Simple text preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49abe5f",
   "metadata": {},
   "source": [
    "### Text preprocessing practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041103a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import WordNetLemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Retain alphabetic words: alpha_only\n",
    "alpha_only = [t for t in lower_tokens if t.isalpha()]\n",
    "\n",
    "# Remove all stop words: no_stops\n",
    "no_stops = [t for t in alpha_only if t not in english_stops]\n",
    "\n",
    "# Instantiate the WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize all tokens into a new list: lemmatized\n",
    "lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]\n",
    "\n",
    "# Create the bag-of-words: bow\n",
    "bow = Counter(lemmatized)\n",
    "\n",
    "# Print the 10 most common tokens\n",
    "print(bow.most_common(10))\n",
    "\n",
    "\"\"\"\n",
    "[('debugging', 40), ('system', 25), ('bug', 17), ('software', 16), ('problem', 15), ('tool', 15), ('computer', 14), \n",
    "('process', 13), ('term', 13), ('debugger', 13)]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fad5e3d",
   "metadata": {},
   "source": [
    "### Introduction to gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4572a2a8",
   "metadata": {},
   "source": [
    "### Creating and querying a corpus with gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4440f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dictionary\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "# Create a Dictionary from the articles: dictionary\n",
    "dictionary = Dictionary(articles)\n",
    "\n",
    "# Select the id for \"computer\": computer_id\n",
    "computer_id = dictionary.token2id.get(\"computer\")\n",
    "\n",
    "# Use computer_id with the dictionary to print the word\n",
    "print(dictionary.get(computer_id))\n",
    "\n",
    "# Create a MmCorpus: corpus\n",
    "corpus = [dictionary.doc2bow(article) for article in articles]\n",
    "\n",
    "# Print the first 10 word ids with their frequency counts from the fifth document\n",
    "print(corpus[4][:10])\n",
    "\n",
    "\"\"\"\n",
    "computer\n",
    "    [(0, 85), (8, 11), (10, 2), (25, 1), (27, 2), (41, 33), (42, 1), (43, 1), (44, 1), (45, 3)]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e90f4a",
   "metadata": {},
   "source": [
    "### Gensim bag-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cab097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fifth document: doc\n",
    "doc = corpus[4]\n",
    "\n",
    "# Sort the doc for frequency: bow_doc\n",
    "bow_doc = sorted(doc, key=lambda w: w[1], reverse=True)\n",
    "\n",
    "# Print the top 5 words of the document alongside the count\n",
    "for word_id, word_count in bow_doc[:5]:\n",
    "    print(dictionary.get(word_id), word_count)\n",
    "    \n",
    "# Create the defaultdict: total_word_count\n",
    "total_word_count = defaultdict(int)\n",
    "for word_id, word_count in itertools.chain.from_iterable(corpus):\n",
    "    total_word_count[word_id] += word_count\n",
    "    \n",
    "\"\"\"\n",
    "engineering 91\n",
    "'' 85\n",
    "reverse 73\n",
    "software 51\n",
    "`` 33\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f2a7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fifth document: doc\n",
    "doc = corpus[4]\n",
    "\n",
    "# Sort the doc for frequency: bow_doc\n",
    "bow_doc = sorted(doc, key=lambda w: w[1], reverse=True)\n",
    "\n",
    "# Print the top 5 words of the document alongside the count\n",
    "for word_id, word_count in bow_doc[:5]:\n",
    "    print(dictionary.get(word_id), word_count)\n",
    "    \n",
    "# Create the defaultdict: total_word_count\n",
    "total_word_count = defaultdict(int)\n",
    "for word_id, word_count in itertools.chain.from_iterable(corpus):\n",
    "    total_word_count[word_id] += word_count\n",
    "\n",
    "# Create a sorted list from the defaultdict: sorted_word_count \n",
    "sorted_word_count = sorted(total_word_count.items(), key=lambda w: w[1], reverse=True) \n",
    "\n",
    "# Print the top 5 words across all documents alongside the count\n",
    "for word_id, word_count in sorted_word_count[:5]:\n",
    "    print(dictionary.get(word_id), word_count)\n",
    "    \n",
    "\"\"\"\n",
    "    engineering 91\n",
    "    '' 85\n",
    "    reverse 73\n",
    "    software 51\n",
    "    `` 33\n",
    "    '' 1006\n",
    "    computer 598\n",
    "    `` 555\n",
    "    software 450\n",
    "    cite 322\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8bbff4",
   "metadata": {},
   "source": [
    "## Tf-idf with gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d7c0e2",
   "metadata": {},
   "source": [
    "### Tf-idf with Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7c41c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new TfidfModel using the corpus: tfidf\n",
    "tfidf = TfidfModel(corpus)\n",
    "\n",
    "# Calculate the tfidf weights of doc: tfidf_weights\n",
    "tfidf_weights = tfidf[doc]\n",
    "\n",
    "# Print the first five weights\n",
    "print(tfidf_weights[:5])\n",
    "\n",
    "\"\"\"\n",
    "[(10, 0.0022676967632877364), (25, 0.004310646654948723), (27, 0.008621293309897447), (42, 0.0054444950365925915), \n",
    "(43, 0.004310646654948723)]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fa14e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new TfidfModel using the corpus: tfidf\n",
    "tfidf = TfidfModel(corpus)\n",
    "\n",
    "# Calculate the tfidf weights of doc: tfidf_weights\n",
    "tfidf_weights = tfidf[doc]\n",
    "\n",
    "# Print the first five weights\n",
    "print(tfidf_weights[:5])\n",
    "\n",
    "# Sort the weights from highest to lowest: sorted_tfidf_weights\n",
    "sorted_tfidf_weights = sorted(tfidf_weights, key=lambda w: w[1], reverse=True)\n",
    "\n",
    "# Print the top 5 weighted words\n",
    "for term_id, weight in sorted_tfidf_weights[:5]:\n",
    "    print(dictionary.get(term_id), weight)\n",
    "    \n",
    "\"\"\"\n",
    "[(10, 0.0022676967632877364), (25, 0.004310646654948723), (27, 0.008621293309897447), (42, 0.0054444950365925915), \n",
    " (43, 0.004310646654948723)]\n",
    "    reverse 0.4987515710425556\n",
    "    infringement 0.1854420793422032\n",
    "    engineering 0.16280628072296138\n",
    "    interoperability 0.12362805289480211\n",
    "    reverse-engineered 0.12362805289480211\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809131dc",
   "metadata": {},
   "source": [
    "# 3. Named-entity recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4279d19",
   "metadata": {},
   "source": [
    "## Named Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c625f97",
   "metadata": {},
   "source": [
    "### NER with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea694e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the article into sentences: sentences\n",
    "sentences = nltk.sent_tokenize(article)\n",
    "\n",
    "# Tokenize each sentence into words: token_sentences\n",
    "token_sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "\n",
    "# Tag each tokenized sentence into parts of speech: pos_sentences\n",
    "pos_sentences = [nltk.pos_tag(sent) for sent in token_sentences] \n",
    "\n",
    "# Create the named entity chunks: chunked_sentences\n",
    "chunked_sentences = nltk.ne_chunk_sents(pos_sentences, binary=True)\n",
    "\n",
    "# Test for stems of the tree with 'NE' tags\n",
    "for sent in chunked_sentences:\n",
    "    for chunk in sent:\n",
    "        if hasattr(chunk, \"label\") and chunk.label() == \"NE\":\n",
    "            print(chunk)\n",
    "\n",
    "\"\"\"\n",
    "    (NE Uber/NNP)\n",
    "    (NE Beyond/NN)\n",
    "    (NE Apple/NNP)\n",
    "    (NE Uber/NNP)\n",
    "    (NE Uber/NNP)\n",
    "    (NE Travis/NNP Kalanick/NNP)\n",
    "    (NE Tim/NNP Cook/NNP)\n",
    "    (NE Apple/NNP)\n",
    "    (NE Silicon/NNP Valley/NNP)\n",
    "    (NE CEO/NNP)\n",
    "    (NE Yahoo/NNP)\n",
    "    (NE Marissa/NNP Mayer/NNP)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bacd069",
   "metadata": {},
   "source": [
    "### Charting practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957af68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the defaultdict: ner_categories\n",
    "ner_categories = defaultdict(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b6561d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the defaultdict: ner_categories\n",
    "ner_categories = defaultdict(int)\n",
    "\n",
    "# Create the nested for loop\n",
    "for sent in chunked_sentences:\n",
    "    for chunk in sent:\n",
    "        if hasattr(chunk, 'label'):\n",
    "            ner_categories[chunk.label()] += 1\n",
    "            \n",
    "# Create a list from the dictionary keys for the chart labels: labels\n",
    "labels = list(ner_categories.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7309218d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the defaultdict: ner_categories\n",
    "ner_categories = defaultdict(int)\n",
    "\n",
    "# Create the nested for loop\n",
    "for sent in chunked_sentences:\n",
    "    for chunk in sent:\n",
    "        if hasattr(chunk, 'label'):\n",
    "            ner_categories[chunk.label()] += 1\n",
    "            \n",
    "# Create a list from the dictionary keys for the chart labels: labels\n",
    "labels = list(ner_categories.keys())\n",
    "\n",
    "# Create a list of the values: values\n",
    "values = [ner_categories.get(v) for v in labels]\n",
    "\n",
    "# Create the pie chart\n",
    "plt.pie(values, labels=labels, autopct='%1.1f%%', startangle=140)\n",
    "\n",
    "# Display the chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc1e1ad",
   "metadata": {},
   "source": [
    "## Introduction to SpaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf21e95",
   "metadata": {},
   "source": [
    "### Comparing NLTK with spaCy NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa3e7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import spacy\n",
    "import spacy\n",
    "\n",
    "# Instantiate the English model: nlp\n",
    "nlp = spacy.load('en_core_web_sm', disable=['tagger', 'parser', 'matcher'])\n",
    "\n",
    "# Create a new document: doc\n",
    "doc = nlp(article)\n",
    "\n",
    "# Print all of the found entities and their labels\n",
    "for ent in doc.ents:\n",
    "    print(ent.label_, ent.text)\n",
    "    \n",
    "\"\"\"\n",
    "    ORG Apple\n",
    "    PERSON Travis Kalanick of Uber\n",
    "    PERSON Tim Cook\n",
    "    ORG Apple\n",
    "    CARDINAL Millions\n",
    "    LOC Silicon Valley\n",
    "    ORG Yahoo\n",
    "    PERSON Marissa Mayer\n",
    "    MONEY 186\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9a3578",
   "metadata": {},
   "source": [
    "## Multilingual NER with polyglot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac00d2d4",
   "metadata": {},
   "source": [
    "### French NER with polyglot I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d7aac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new text object using Polyglot's Text class: txt\n",
    "txt = Text(article)\n",
    "\n",
    "# Print each of the entities found\n",
    "for ent in txt.entities:\n",
    "    print(ent)\n",
    "    \n",
    "# Print the type of ent\n",
    "print(type(ent))\n",
    "\n",
    "\"\"\"\n",
    "    ['Charles', 'Cuvelliez']\n",
    "    ['Charles', 'Cuvelliez']\n",
    "    ['Bruxelles']\n",
    "    ['l‚ÄôIA']\n",
    "    ['Julien', 'Maldonato']\n",
    "    ['Deloitte']\n",
    "    ['Ethiquement']\n",
    "    ['l‚ÄôIA']\n",
    "    ['.']\n",
    "    <class 'polyglot.text.Chunk'>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3bbc92",
   "metadata": {},
   "source": [
    "### French NER with polyglot II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096034b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the list of tuples: entities\n",
    "entities = [(ent.tag, ' '.join(ent)) for ent in txt.entities]\n",
    "\n",
    "# Print entities\n",
    "print(entities)\n",
    "\n",
    "\"\"\"\n",
    "[('I-PER', 'Charles Cuvelliez'), ('I-PER', 'Charles Cuvelliez'), ('I-ORG', 'Bruxelles'), ('I-PER', 'l‚ÄôIA'), \n",
    "('I-PER', 'Julien Maldonato'), \n",
    "('I-ORG', 'Deloitte'), ('I-PER', 'Ethiquement'), ('I-LOC', 'l‚ÄôIA'), ('I-PER', '.')]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a647630",
   "metadata": {},
   "source": [
    "### Spanish NER with polyglot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d99e27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the count variable: count\n",
    "count = 0\n",
    "\n",
    "# Iterate over all the entities\n",
    "for ent in txt.entities:\n",
    "    # Check whether the entity contains 'M√°rquez' or 'Gabo'\n",
    "    if \"M√°rquez\" in ent or \"Gabo\" in ent:\n",
    "        # Increment count\n",
    "        count += 1\n",
    "\n",
    "# Print count\n",
    "print(count)\n",
    "\n",
    "# Calculate the percentage of entities that refer to \"Gabo\": percentage\n",
    "percentage = count / len(txt.entities)\n",
    "print(percentage)\n",
    "\n",
    "\"\"\"\n",
    "['Lina']\n",
    "['Castillo']\n",
    "['Teresa', 'Lozano', 'Long']\n",
    "['Universidad', 'de', 'Texas']\n",
    "['Austin']\n",
    "['Austin', '.']\n",
    "['Austin', '.', 'Ella']\n",
    "['Gabriel', 'Garc√≠a', 'M√°rquez']\n",
    "['Gabriel', 'Garc√≠a', 'M√°rquez']\n",
    "['LIna']\n",
    "['Castillo']\n",
    "['colombiano']\n",
    "['Colombia']\n",
    "['Estados', 'Unidos']\n",
    "['Castillo']\n",
    "['Nation']\n",
    "['Kenneth', 'Nebenzahl']\n",
    "['Jr']\n",
    "['Library']\n",
    "['Society']\n",
    "['Humboldt']\n",
    "['Am√©rica', 'Latina']\n",
    "['.', 'Garc√≠a', 'M√°rquez']\n",
    "['colombiano']\n",
    "['Gabriel', 'Garc√≠a', 'M√°rquez']\n",
    "['Gabo']\n",
    "['Fidel', 'Castro']\n",
    "['Gabo']\n",
    "['Castro']\n",
    "['colombianos']\n",
    "['Gabo']\n",
    "['colombiano']\n",
    "['colombiano', 'Belisario', 'Betancur']\n",
    "['Betancur']\n",
    "['Fuerzas', 'Armadas', 'Revolucionarias', 'de', 'Colombia']\n",
    "['FARC']\n",
    "['Gabo']\n",
    "['Cuba']\n",
    "['Betancur']\n",
    "['Gabo']\n",
    "['Cuba']\n",
    "['.', 'Betancur']\n",
    "['Gabriel', 'Garc√≠a', 'M√°rquez']\n",
    "['Gabo']\n",
    "['San', 'Vicente', 'del', 'Cagu√°n']\n",
    "['Andr√©s', 'Patrana']\n",
    "['FARC']\n",
    "['Gabo']\n",
    "['Pastrana']\n",
    "['Ej√©rcito', 'de', 'Liberaci√≥n', 'Nacional']\n",
    "['ELN']\n",
    "['Garc√≠a', 'M√°rquez']\n",
    "['Mercedes', 'Barcha']\n",
    "['ELN']\n",
    "['La', 'Habana']\n",
    "['Gabo']\n",
    "['La', 'Habana']\n",
    "['Gabo']\n",
    "['Fidel', 'Castro']\n",
    "['Gabriel', 'Garc√≠a', 'M√°rquez']\n",
    "['Fidel', 'Castro']\n",
    "['Gabriel', 'Garc√≠a', 'M√°rquez']\n",
    "['Carmen', 'Balcells']\n",
    "['La', 'Habana']\n",
    "['Estados', 'Unidos']\n",
    "['Colombia']\n",
    "['.', 'Gabriel', 'Garc√≠a', 'M√°rquez']\n",
    "['Gabo']\n",
    "['Salman', 'Rushdie']\n",
    "['Elena', 'Poniatowska']\n",
    "['Gabo']\n",
    "['Gabo']\n",
    "['Gabriel', 'Garc√≠a', 'M√°rquez']\n",
    "['Garc√≠a', 'M√°rquez']\n",
    "['Macondo']\n",
    "['colombiana']\n",
    "['United', 'Fruit', 'Company']\n",
    "['colombiano']\n",
    "['Ci√©naga']\n",
    "['Santa']\n",
    "['Santa', 'Marta']\n",
    "['Garc√≠a', 'M√°rquez']\n",
    "['York']\n",
    "['Espa√±ol']\n",
    "['Jos√©', 'Arcadio', 'Segundo']\n",
    "['Garc√≠a', 'M√°rquez']\n",
    "['United', 'Fruit']\n",
    "['Colombia']\n",
    "['Macondo']\n",
    "['Gabo']\n",
    "['Gabo']\n",
    "['Ci√©naga']\n",
    "['colombiana']\n",
    "['Jos√©', 'Arcadio', 'Segundo']\n",
    "['Garc√≠a', 'M√°rquez']\n",
    "['Gabriel', 'Garc√≠a', 'M√°rquez']\n",
    "['Harry', 'Ransom']\n",
    "['Harry', 'Ransom', 'Center']\n",
    "\n",
    "<script.py> output:\n",
    "    29\n",
    "    0.29591836734693877\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd9fbf8",
   "metadata": {},
   "source": [
    "# 4. Building a \"fake news\" classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee7c683",
   "metadata": {},
   "source": [
    "## Classifying fake news using supervised learning with NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4a7fd3",
   "metadata": {},
   "source": [
    "## Building word count vectors with scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6122be55",
   "metadata": {},
   "source": [
    "### CountVectorizer for text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c85211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Print the head of df\n",
    "print(df.head())\n",
    "\n",
    "# Create a series to store the labels: y\n",
    "y = df.label\n",
    "\n",
    "# Create training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], y, test_size=0.33, random_state=53)\n",
    "\n",
    "# Initialize a CountVectorizer object: count_vectorizer\n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "# Transform the training data using only the 'text' column values: count_train \n",
    "count_train = count_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data using only the 'text' column values: count_test \n",
    "count_test = count_vectorizer.transform(X_test)\n",
    "\n",
    "# Print the first 10 features of the count_vectorizer\n",
    "print(count_vectorizer.get_feature_names()[:10])\n",
    "\n",
    "\"\"\"\n",
    "       Unnamed: 0                                              title                                               text label\n",
    "    0        8476                       You Can Smell Hillary‚Äôs Fear  Daniel Greenfield, a Shillman Journalism Fello...  FAKE\n",
    "    1       10294  Watch The Exact Moment Paul Ryan Committed Pol...  Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE\n",
    "    2        3608        Kerry to go to Paris in gesture of sympathy  U.S. Secretary of State John F. Kerry said Mon...  REAL\n",
    "    3       10142  Bernie supporters on Twitter erupt in anger ag...  ‚Äî Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE\n",
    "    4         875   The Battle of New York: Why This Primary Matters  It's primary day in New York and front-runners...  REAL\n",
    "    ['00', '000', '0000', '00000031', '000035', '00006', '0001', '0001pt', '000ft', '000km']\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a252f4",
   "metadata": {},
   "source": [
    "### TfidfVectorizer for text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d7f841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize a TfidfVectorizer object: tfidf_vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_df=0.7)\n",
    "\n",
    "# Transform the training data: tfidf_train \n",
    "tfidf_train = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data: tfidf_test \n",
    "tfidf_test = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Print the first 10 features\n",
    "print(tfidf_vectorizer.get_feature_names()[:10])\n",
    "\n",
    "# Print the first 5 vectors of the tfidf training data\n",
    "print(tfidf_train.A[:5])\n",
    "\n",
    "\"\"\"\n",
    "    ['00', '000', '001', '008s', '00am', '00pm', '01', '01am', '02', '024']\n",
    "    [[0.         0.01928563 0.         ... 0.         0.         0.        ]\n",
    "     [0.         0.         0.         ... 0.         0.         0.        ]\n",
    "     [0.         0.02895055 0.         ... 0.         0.         0.        ]\n",
    "     [0.         0.03056734 0.         ... 0.         0.         0.        ]\n",
    "     [0.         0.         0.         ... 0.         0.         0.        ]]\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef692922",
   "metadata": {},
   "source": [
    "### Inspecting the vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3ad2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the CountVectorizer DataFrame: count_df\n",
    "count_df = pd.DataFrame(count_train.A, columns=count_vectorizer.get_feature_names())\n",
    "\n",
    "# Create the TfidfVectorizer DataFrame: tfidf_df\n",
    "tfidf_df = pd.DataFrame(tfidf_train.A, columns=tfidf_vectorizer.get_feature_names())\n",
    "\n",
    "# Print the head of count_df\n",
    "print(count_df.head())\n",
    "\n",
    "# Print the head of tfidf_df\n",
    "print(tfidf_df.head())\n",
    "\n",
    "# Calculate the difference in columns: difference\n",
    "difference = set(count_df.columns) - set(tfidf_df.columns)\n",
    "print(difference)\n",
    "\n",
    "# Check whether the DataFrames are equal\n",
    "print(count_df.equals(tfidf_df))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "       000  00am  0600  10  100  ...  yuan  zawahiri  zeitung  zero  zerohedge\n",
    "    0    0     0     0   0    0  ...     0         0        0     1          0\n",
    "    1    0     0     0   3    0  ...     0         0        0     0          0\n",
    "    2    0     0     0   0    0  ...     0         0        0     0          0\n",
    "    3    0     0     0   0    0  ...     0         0        0     0          0\n",
    "    4    0     0     0   0    0  ...     0         0        0     0          0\n",
    "    \n",
    "    [5 rows x 5111 columns]\n",
    "       000  00am  0600     10  100  ...  yuan  zawahiri  zeitung   zero  zerohedge\n",
    "    0  0.0   0.0   0.0  0.000  0.0  ...   0.0       0.0      0.0  0.034        0.0\n",
    "    1  0.0   0.0   0.0  0.106  0.0  ...   0.0       0.0      0.0  0.000        0.0\n",
    "    2  0.0   0.0   0.0  0.000  0.0  ...   0.0       0.0      0.0  0.000        0.0\n",
    "    3  0.0   0.0   0.0  0.000  0.0  ...   0.0       0.0      0.0  0.000        0.0\n",
    "    4  0.0   0.0   0.0  0.000  0.0  ...   0.0       0.0      0.0  0.000        0.0\n",
    "    \n",
    "    [5 rows x 5111 columns]\n",
    "    set()\n",
    "    False\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691ecc2a",
   "metadata": {},
   "source": [
    "## Training and testing a classification model with scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2389de13",
   "metadata": {},
   "source": [
    "### Training and testing the \"fake news\" model with CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38441f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "# Instantiate a Multinomial Naive Bayes classifier: nb_classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "nb_classifier.fit(count_train, y_train)\n",
    "\n",
    "# Create the predicted tags: pred\n",
    "pred = nb_classifier.predict(count_test)\n",
    "\n",
    "# Calculate the accuracy score: score\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(score)\n",
    "\n",
    "# Calculate the confusion matrix: cm\n",
    "cm = metrics.confusion_matrix(y_test, pred, labels=['FAKE', 'REAL'])\n",
    "print(cm)\n",
    "\n",
    "\"\"\"\n",
    "    0.893352462936394\n",
    "    [[ 865  143]\n",
    "     [  80 1003]]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c5becf",
   "metadata": {},
   "source": [
    "### Training and testing the \"fake news\" model with TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9e6421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Multinomial Naive Bayes classifier: nb_classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "nb_classifier.fit(tfidf_train, y_train)\n",
    "\n",
    "# Create the predicted tags: pred\n",
    "pred = nb_classifier.predict(tfidf_test)\n",
    "\n",
    "# Calculate the accuracy score: score\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(score)\n",
    "\n",
    "# Calculate the confusion matrix: cm\n",
    "cm = metrics.confusion_matrix(y_test, pred, labels=['FAKE', 'REAL'])\n",
    "print(cm)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    0.8565279770444764\n",
    "    [[ 739  269]\n",
    "     [  31 1052]]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d259bd2a",
   "metadata": {},
   "source": [
    "## Simple NLP, complex problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a142c4",
   "metadata": {},
   "source": [
    "### Improving your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7df30b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the list of alphas: alphas\n",
    "alphas = np.arange(0, 1, .1)\n",
    "\n",
    "# Define train_and_predict()\n",
    "def train_and_predict(alpha):\n",
    "    # Instantiate the classifier: nb_classifier\n",
    "    nb_classifier = MultinomialNB(alpha=alpha)\n",
    "    # Fit to the training data\n",
    "    nb_classifier.fit(tfidf_train, y_train)\n",
    "    # Predict the labels: pred\n",
    "    pred = nb_classifier.predict(tfidf_test)\n",
    "    # Compute accuracy: score\n",
    "    score = metrics.accuracy_score(y_test, pred)\n",
    "    return score\n",
    "\n",
    "# Iterate over the alphas and print the corresponding score\n",
    "for alpha in alphas:\n",
    "    print('Alpha: ', alpha)\n",
    "    print('Score: ', train_and_predict(alpha))\n",
    "    print()\n",
    "    \n",
    "\"\"\"\n",
    "    Alpha:  0.0\n",
    "    Score:  0.8813964610234337\n",
    "    \n",
    "    Alpha:  0.1\n",
    "    Score:  0.8976566236250598\n",
    "    \n",
    "    Alpha:  0.2\n",
    "    Score:  0.8938307030129125\n",
    "    \n",
    "    Alpha:  0.30000000000000004\n",
    "    Score:  0.8900047824007652\n",
    "    \n",
    "    Alpha:  0.4\n",
    "    Score:  0.8857006217120995\n",
    "    \n",
    "    Alpha:  0.5\n",
    "    Score:  0.8842659014825442\n",
    "    \n",
    "    Alpha:  0.6000000000000001\n",
    "    Score:  0.874701099952176\n",
    "    \n",
    "    Alpha:  0.7000000000000001\n",
    "    Score:  0.8703969392635102\n",
    "    \n",
    "    Alpha:  0.8\n",
    "    Score:  0.8660927785748446\n",
    "    \n",
    "    Alpha:  0.9\n",
    "    Score:  0.8589191774270684\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22624c3c",
   "metadata": {},
   "source": [
    "### Inspecting your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2192d052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the class labels: class_labels\n",
    "class_labels = nb_classifier.classes_\n",
    "\n",
    "# Extract the features: feature_names\n",
    "feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "# Zip the feature names together with the coefficient array and sort by weights: feat_with_weights\n",
    "feat_with_weights = sorted(zip(nb_classifier.coef_[0], feature_names))\n",
    "\n",
    "# Print the first class label and the top 20 feat_with_weights entries\n",
    "print(class_labels[0], feat_with_weights[:20])\n",
    "\n",
    "# Print the second class label and the bottom 20 feat_with_weights entries\n",
    "print(class_labels[1], feat_with_weights[-20:])\n",
    "\n",
    "\n",
    "\"\"\" \n",
    "    FAKE [(-12.641778440826338, '0000'), (-12.641778440826338, '000035'), (-12.641778440826338, '0001'), \n",
    "    (-12.641778440826338, '0001pt'), (-12.641778440826338, '000km'), (-12.641778440826338, '0011'), \n",
    "    (-12.641778440826338, '006s'), (-12.641778440826338, '007'), (-12.641778440826338, '007s'), (-12.641778440826338, '008s'), \n",
    "    (-12.641778440826338, '0099'), (-12.641778440826338, '00am'), (-12.641778440826338, '00p'), (-12.641778440826338, '00pm'), \n",
    "    (-12.641778440826338, '014'), (-12.641778440826338, '015'), (-12.641778440826338, '018'), (-12.641778440826338, '01am'), \n",
    "    (-12.641778440826338, '020'), (-12.641778440826338, '023')]\n",
    "    \n",
    "    REAL [(-6.790929954967984, 'states'), (-6.765360557845787, 'rubio'), (-6.751044290367751, 'voters'), \n",
    "    (-6.701050756752027, 'house'), (-6.695547793099875, 'republicans'), (-6.670191249042969, 'bush'), \n",
    "    (-6.661945235816139, 'percent'), (-6.589623788689861, 'people'), (-6.559670340096453, 'new'), \n",
    "    (-6.489892292073902, 'party'), (-6.452319082422527, 'cruz'), (-6.452076515575875, 'state'), \n",
    "    (-6.397696648238072, 'republican'), (-6.376343060363355, 'campaign'), (-6.324397735392007, 'president'), \n",
    "    (-6.2546017970213645, 'sanders'), (-6.144621899738043, 'obama'), (-5.756817248152807, 'clinton'), \n",
    "    (-5.596085785733112, 'said'), (-5.357523914504495, 'trump')]\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
