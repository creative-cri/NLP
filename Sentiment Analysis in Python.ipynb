{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33bc88d9",
   "metadata": {},
   "source": [
    "# Sentiment Analysis in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247ca674",
   "metadata": {},
   "source": [
    "# 1. Sentiment Analysis Nuts and Bolts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5f2ffc",
   "metadata": {},
   "source": [
    "## Welcome!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae492cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the number of positive and negative reviews\n",
    "print('Number of positive and negative reviews: ', movies.label.value_counts())\n",
    "\n",
    "# Find the proportion of positive and negative reviews\n",
    "print('Proportion of positive and negative reviews: ', movies.label.value_counts() / len(movies))\n",
    "\n",
    "\"\"\"\n",
    "Number of positive and negative reviews:  0    530\n",
    "    1    470\n",
    "    Name: label, dtype: int64\n",
    "    Proportion of positive and negative reviews:  0    0.53\n",
    "    1    0.47\n",
    "    Name: label, dtype: float64\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2773aa9b",
   "metadata": {},
   "source": [
    "### Longest and shortest reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9867ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "length_reviews = movies.review.str.len()\n",
    "\n",
    "# How long is the shortest review\n",
    "print(min(length_reviews))\n",
    "\n",
    "# 53"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c86f8b7",
   "metadata": {},
   "source": [
    "## Sentiment analysis types and approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301a02da",
   "metadata": {},
   "source": [
    "### Detecting the sentiment of Tale of Two Cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e77b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required packages\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Create a textblob object \n",
    "blob_two_cities = TextBlob(two_cities)\n",
    "\n",
    "# Print out the sentiment   \n",
    "print(blob_two_cities.sentiment)\n",
    "\n",
    "# Sentiment(polarity=0.022916666666666658, subjectivity=0.5895833333333332)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14aa3a8e",
   "metadata": {},
   "source": [
    "### Comparing the sentiment of two strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07938ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required packages\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Create a textblob object \n",
    "blob_annak = TextBlob(annak)\n",
    "blob_catcher = TextBlob(catcher)\n",
    "\n",
    "# Print out the sentiment   \n",
    "print('Sentiment of annak: ', blob_annak.sentiment)\n",
    "print('Sentiment of catcher: ', blob_catcher.sentiment)\n",
    "\n",
    "# Sentiment of annak:  Sentiment(polarity=0.05000000000000002, subjectivity=0.95)\n",
    "# Sentiment of catcher:  Sentiment(polarity=-0.05, subjectivity=0.5466666666666666)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acd8058",
   "metadata": {},
   "source": [
    "### What is the sentiment of a movie review?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82886f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required packages\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Create a textblob object \n",
    "blob_titanic = TextBlob(titanic)\n",
    "\n",
    "# Print out its sentiment\n",
    "print(blob_titanic.sentiment)\n",
    "\n",
    "# Sentiment(polarity=0.2024748060772906, subjectivity=0.4518248900857597)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4cca6b",
   "metadata": {},
   "source": [
    "## Let's build a word cloud!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eec11fb",
   "metadata": {},
   "source": [
    "### Your first word cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dee480a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "# Generate the word cloud from the east_of_eden string\n",
    "cloud_east_of_eden = WordCloud(background_color=\"white\").generate(east_of_eden)\n",
    "\n",
    "# Create a figure of the generated cloud\n",
    "plt.imshow(cloud_east_of_eden, interpolation='bilinear')  \n",
    "plt.axis('off')\n",
    "# Display the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4c033f",
   "metadata": {},
   "source": [
    "### Word Cloud on movie reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603a8a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the word cloud function \n",
    "from wordcloud import WordCloud \n",
    "\n",
    "# Create and generate a word cloud image\n",
    "my_cloud = WordCloud(background_color='white', stopwords=my_stopwords).generate(descriptions)\n",
    "\n",
    "# Display the generated wordcloud image\n",
    "plt.imshow(my_cloud, interpolation='bilinear') \n",
    "plt.axis(\"off\")\n",
    "\n",
    "# Don't forget to show the final image\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65486ea3",
   "metadata": {},
   "source": [
    "# 2. Numeric Features from Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453dcc71",
   "metadata": {},
   "source": [
    "## Bag-of-words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3914c93",
   "metadata": {},
   "source": [
    "### Your first BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24dd324a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required function\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "annak = ['Happy families are all alike;', 'every unhappy family is unhappy in its own way']\n",
    "\n",
    "# Build the vectorizer and fit it\n",
    "anna_vect = CountVectorizer()\n",
    "anna_vect.fit(annak)\n",
    "\n",
    "# Create the bow representation\n",
    "anna_bow = anna_vect.transform(annak)\n",
    "\n",
    "# Print the bag-of-words result \n",
    "print(anna_bow.toarray())\n",
    "\n",
    "\"\"\"\n",
    "[[1 1 1 0 1 0 1 0 0 0 0 0 0]\n",
    "[0 0 0 1 0 1 0 1 1 1 1 2 1]]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1d8e9d",
   "metadata": {},
   "source": [
    "### BOW using product reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2141dd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "# Build the vectorizer, specify max features \n",
    "vect = CountVectorizer(max_features=100)\n",
    "# Fit the vectorizer\n",
    "vect.fit(reviews.review)\n",
    "\n",
    "# Transform the review column\n",
    "X_review = vect.transform(reviews.review)\n",
    "\n",
    "# Create the bow representation\n",
    "X_df = pd.DataFrame(X_review.toarray(), columns=vect.get_feature_names())\n",
    "print(X_df.head())\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "       about  after  all  also  am  ...  with  work  would  you  your\n",
    "    0      0      0    1     0   0  ...     1     0      2    0     1\n",
    "    1      0      0    0     0   0  ...     0     0      1    1     0\n",
    "    2      0      0    3     0   0  ...     0     1      1    2     0\n",
    "    3      0      0    0     0   0  ...     0     0      0    0     0\n",
    "    4      0      1    0     0   0  ...     0     0      0    3     1\n",
    "    \n",
    "    [5 rows x 100 columns]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94dfb7f",
   "metadata": {},
   "source": [
    "## Getting granular with n-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e10d4f0",
   "metadata": {},
   "source": [
    "### Specify token sequence length with BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46db19c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "# Build the vectorizer, specify token sequence and fit\n",
    "vect = CountVectorizer(ngram_range=(1, 2))\n",
    "vect.fit(reviews.review)\n",
    "\n",
    "# Transform the review column\n",
    "X_review = vect.transform(reviews.review)\n",
    "\n",
    "# Create the bow representation\n",
    "X_df = pd.DataFrame(X_review.toarray(), columns=vect.get_feature_names())\n",
    "print(X_df.head())\n",
    "\n",
    "\n",
    "\"\"\" \n",
    "       10  10 95  10 cups  100  100 years  ...  zelbessdisk  zelbessdisk three  zen  zen baseball  zen motorcycle\n",
    "    0   0      0        0    0          0  ...            0                  0    0             0               0\n",
    "    1   0      0        0    0          0  ...            0                  0    0             0               0\n",
    "    2   0      0        0    0          0  ...            0                  0    0             0               0\n",
    "    3   0      0        0    0          0  ...            1                  1    0             0               0\n",
    "    4   0      0        0    0          0  ...            0                  0    0             0               0\n",
    "    \n",
    "    [5 rows x 8436 columns]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba927211",
   "metadata": {},
   "source": [
    "### Size of vocabulary of movies reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d89cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "# Build the vectorizer, specify size of vocabulary and fit\n",
    "vect = CountVectorizer(max_features=100)\n",
    "vect.fit(movies.review)\n",
    "\n",
    "# Transform the review column\n",
    "X_review = vect.transform(movies.review)\n",
    "# Create the bow representation\n",
    "X_df = pd.DataFrame(X_review.toarray(), columns=vect.get_feature_names())\n",
    "print(X_df.head())\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "       about  all  also  an  and  ...  who  will  with  would  you\n",
    "    0      0    0     0   0    1  ...    0     0     1      1    0\n",
    "    1      0    3     1   1   11  ...    0     2     7      2    3\n",
    "    2      0    0     0   1    7  ...    0     0     2      0    0\n",
    "    3      0    0     0   2    1  ...    1     0     0      0    1\n",
    "    4      0    3     0   0    8  ...    0     0     2      0    0\n",
    "    \n",
    "    [5 rows x 100 columns]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91fd252",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "# Build and fit the vectorizer\n",
    "vect = CountVectorizer(max_df=200)\n",
    "vect.fit(movies.review)\n",
    "\n",
    "# Transform the review column\n",
    "X_review = vect.transform(movies.review)\n",
    "# Create the bow representation\n",
    "X_df = pd.DataFrame(X_review.toarray(), columns=vect.get_feature_names())\n",
    "print(X_df.head())\n",
    "\n",
    "\"\"\"\n",
    "       00  000  000s  007  00s  ...  zooms  zsigmond  zulu  zuniga  zvyagvatsev\n",
    "    0   0    0     0    0    0  ...      0         0     0       0            0\n",
    "    1   0    0     0    0    0  ...      0         0     0       0            0\n",
    "    2   0    0     0    0    0  ...      0         0     0       0            0\n",
    "    3   0    0     0    0    0  ...      0         0     0       0            0\n",
    "    4   0    0     0    0    0  ...      0         0     0       0            0\n",
    "    \n",
    "    [5 rows x 17669 columns]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353863d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "# Build and fit the vectorizer\n",
    "vect = CountVectorizer(min_df=50)\n",
    "vect.fit(movies.review)\n",
    "\n",
    "# Transform the review column\n",
    "X_review = vect.transform(movies.review)\n",
    "# Create the bow representation\n",
    "X_df = pd.DataFrame(X_review.toarray(), columns=vect.get_feature_names())\n",
    "print(X_df.head())\n",
    "\n",
    "\"\"\"\n",
    "       10  about  absolutely  acting  action  ...  yes  yet  you  young  your\n",
    "    0   0      0           0       0       0  ...    0    0    0      0     0\n",
    "    1   1      0           0       1       0  ...    0    1    3      0     2\n",
    "    2   0      0           0       0       0  ...    0    0    0      1     0\n",
    "    3   0      0           0       0       1  ...    0    0    1      1     0\n",
    "    4   1      0           0       0       1  ...    0    0    0      0     0\n",
    "    \n",
    "    [5 rows x 434 columns]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a057cb58",
   "metadata": {},
   "source": [
    "### BOW with n-grams and vocabulary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a62e9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "# Build the vectorizer, specify max features and fit\n",
    "vect = CountVectorizer(max_features=1000, ngram_range=(2, 2), max_df=500)\n",
    "vect.fit(reviews.review)\n",
    "\n",
    "# Transform the review\n",
    "X_review = vect.transform(reviews.review)\n",
    "\n",
    "# Create a DataFrame from the bow representation\n",
    "X_df = pd.DataFrame(X_review.toarray(), columns=vect.get_feature_names())\n",
    "print(X_df.head())\n",
    "\n",
    "\"\"\"\n",
    "       1980 style  aa batteries  aaa batteries  able to  about the  ...  you want  you will  your imagination  your money  yr old\n",
    "    0           0             0              0        0          0  ...         0         0                 0           0       0\n",
    "    1           0             0              0        0          0  ...         0         0                 0           0       0\n",
    "    2           0             0              0        0          0  ...         0         0                 0           0       0\n",
    "    3           0             0              0        0          0  ...         0         0                 0           0       0\n",
    "    4           0             0              0        0          0  ...         0         0                 0           0       0\n",
    "    \n",
    "    [5 rows x 1000 columns]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1416233",
   "metadata": {},
   "source": [
    "## Build new features from text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67f42bf",
   "metadata": {},
   "source": [
    "### Tokenize a string from GoT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f4803c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required function\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# Transform the GoT string to word tokens\n",
    "print(word_tokenize(GoT))\n",
    "\n",
    "\"\"\"\n",
    "['Never', 'forget', 'what', 'you', 'are', ',', 'for', 'surely', 'the', 'world', 'will', 'not', '.', 'Make', 'it', 'your', \n",
    "'strength', '.', 'Then', 'it', 'can', 'never', 'be', 'your', 'weakness', '.', 'Armour', 'yourself', 'in', 'it', ',', 'and',\n",
    "'it', 'will', 'never', 'be', 'used', 'to', 'hurt', 'you', '.']\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a954dfd8",
   "metadata": {},
   "source": [
    "### Word tokens from the Avengers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da9f206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the word tokenizing function\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# Tokenize each item in the avengers \n",
    "tokens_avengers = [word_tokenize(item) for item in avengers]\n",
    "\n",
    "print(tokens_avengers)\n",
    "\n",
    "\"\"\"\n",
    "[['Cause', 'if', 'we', 'ca', \"n't\", 'protect', 'the', 'Earth', ',', 'you', 'can', 'be', 'd', '*', '*', '*',\n",
    "'sure', 'we', \"'ll\", 'avenge', 'it'], ['There', 'was', 'an', 'idea', 'to', 'bring', 'together', 'a', 'group', \n",
    "'of', 'remarkable', 'people', ',', 'to', 'see', 'if', 'we', 'could', 'become', 'something', 'more'], ['These', \n",
    "'guys', 'come', 'from', 'legend', ',', 'Captain', '.', 'They', \"'re\", 'basically', 'Gods', '.']]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd041733",
   "metadata": {},
   "source": [
    "### A feature for the length of a review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb22f4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the needed packages\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# Tokenize each item in the review column\n",
    "word_tokens = [word_tokenize(review) for review in reviews.review]\n",
    "\n",
    "# Print out the first item of the word_tokens list\n",
    "print(word_tokens[0])\n",
    "\n",
    "\"\"\"\n",
    "['Stuning', 'even', 'for', 'the', 'non-gamer', ':', 'This', 'sound', 'track', 'was', 'beautiful', '!', 'It', 'paints', \n",
    "'the', 'senery', 'in', 'your', 'mind', 'so', 'well', 'I', 'would', 'recomend', 'it', 'even', 'to', 'people', 'who', \n",
    "'hate', 'vid', '.', 'game', 'music', '!', 'I', 'have', 'played', 'the', 'game', 'Chrono', 'Cross', 'but', 'out', 'of', \n",
    "'all', 'of', 'the', 'games', 'I', 'have', 'ever', 'played', 'it', 'has', 'the', 'best', 'music', '!', 'It', 'backs', \n",
    "'away', 'from', 'crude', 'keyboarding', 'and', 'takes', 'a', 'fresher', 'step', 'with', 'grate', 'guitars', 'and', \n",
    "'soulful', 'orchestras', '.', 'It', 'would', 'impress', 'anyone', 'who', 'cares', 'to', 'listen', '!', '^_^']\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f61f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to store the length of the reviews\n",
    "len_tokens = []\n",
    "\n",
    "# Iterate over the word_tokens list and determine the length of each item\n",
    "for i in range(len(word_tokens)):\n",
    "     len_tokens.append(len(word_tokens[i]))\n",
    "\n",
    "# Create a new feature for the lengh of each review\n",
    "reviews['n_words'] = len_tokens "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5fd192",
   "metadata": {},
   "source": [
    "## Can you guess the language?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39f5678",
   "metadata": {},
   "source": [
    "### Identify the language of a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3291c80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the language detection function and package\n",
    "from langdetect import detect_langs\n",
    "\n",
    "# Detect the language of the foreign string\n",
    "print(detect_langs(foreign))\n",
    "\n",
    "# [fr:0.9999972065813575]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bce14a",
   "metadata": {},
   "source": [
    "### Detect language of a list of strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3a156f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect_langs\n",
    "\n",
    "languages = []\n",
    "\n",
    "# Loop over the sentences in the list and detect their language\n",
    "for sentence in sentences:\n",
    "    languages.append(detect_langs(sentence))\n",
    "    \n",
    "print('The detected languages are: ', languages)\n",
    "\n",
    "\"\"\"\n",
    "[\"L'histoire rendu était fidèle, excellent, et grande.\", 'Excelente muy recomendable.', \n",
    "'It had a leak from day one but the return and exchange process was very quick.']\n",
    "\n",
    "\n",
    "The detected languages are:  [[fr:0.9999970274265436], [es:0.9999954111285407], [en:0.999997772943068]]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b0f294",
   "metadata": {},
   "source": [
    "### Language detection of product reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc521a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect_langs\n",
    "languages = [] \n",
    "\n",
    "# Loop over the rows of the dataset and append  \n",
    "for row in range(len(non_english_reviews)):\n",
    "    languages.append(detect_langs(non_english_reviews.iloc[row, 1]))\n",
    "\n",
    "# Clean the list by splitting     \n",
    "languages = [str(lang).split(':')[0][1:] for lang in languages]\n",
    "\n",
    "# Assign the list to a new feature \n",
    "non_english_reviews['language'] = languages\n",
    "\n",
    "print(non_english_reviews.head())\n",
    "\n",
    "\"\"\"\n",
    "          score                                             review language\n",
    "    1249      1   Il grande ritorno!: E' dai tempi del tour di ...       it\n",
    "    1259      1   La reencarnación vista por un científico: El ...       es\n",
    "    1260      1   Excelente Libro / Amazing book!!: Este libro ...       es\n",
    "    1261      1   Magnifico libro: Brian Weiss ha dejado una ma...       es\n",
    "    1639      1   El libro mas completo que existe para nosotra...       es\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a196f59",
   "metadata": {},
   "source": [
    "# 3. More on Numeric Vectors: Transforming Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a85ec8",
   "metadata": {},
   "source": [
    "## Stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9495bd4",
   "metadata": {},
   "source": [
    "### Word cloud of tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc652e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the word cloud function \n",
    "from wordcloud import WordCloud \n",
    "\n",
    "# Create and generate a word cloud image\n",
    "my_cloud = WordCloud(background_color='white').generate(text_tweet)\n",
    "\n",
    "# Display the generated wordcloud image\n",
    "plt.imshow(my_cloud, interpolation='bilinear') \n",
    "plt.axis(\"off\")\n",
    "\n",
    "# Don't forget to show the final image\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ee7dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the word cloud function and stop words list\n",
    "from wordcloud import WordCloud, STOPWORDS \n",
    "\n",
    "# Define the list of stopwords\n",
    "my_stop_words = STOPWORDS.update(['airline', 'airplane'])\n",
    "\n",
    "# Create and generate a word cloud image\n",
    "my_cloud = WordCloud(stopwords=my_stop_words).generate(text_tweet)\n",
    "\n",
    "# Display the generated wordcloud image\n",
    "plt.imshow(my_cloud, interpolation='bilinear') \n",
    "plt.axis(\"off\")\n",
    "# Don't forget to show the final image\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb086860",
   "metadata": {},
   "source": [
    "### Airline sentiment with stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e1211c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the stop words\n",
    "from sklearn.feature_extraction.text import CountVectorizer, ENGLISH_STOP_WORDS \n",
    "\n",
    "# Define the stop words\n",
    "my_stop_words = ENGLISH_STOP_WORDS.union(['airline', 'airlines', '@'])\n",
    "\n",
    "# Build and fit the vectorizer\n",
    "vect = CountVectorizer(stop_words=my_stop_words)\n",
    "vect.fit(tweets.text)\n",
    "\n",
    "# Create the bow representation\n",
    "X_review = vect.transform(tweets.text)\n",
    "# Create the data frame\n",
    "X_df = pd.DataFrame(X_review.toarray(), columns=vect.get_feature_names())\n",
    "print(X_df.head())\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "       00  000  000114  000419  0011  ...  zero  zfqmpgxvs6  zone  zsuztnaijq  zv2pt6trk9\n",
    "    0   0    0       0       0     0  ...     0           0     0           0           0\n",
    "    1   0    0       0       0     0  ...     0           0     0           0           0\n",
    "    2   0    0       0       0     0  ...     0           0     0           0           0\n",
    "    3   0    0       0       0     0  ...     0           0     0           0           0\n",
    "    4   0    0       0       0     0  ...     0           0     0           0           0\n",
    "    \n",
    "    [5 rows x 2867 columns]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a36539",
   "metadata": {},
   "source": [
    "### Multiple text columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6658fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the vectorizer and default English stop words list\n",
    "from sklearn.feature_extraction.text import CountVectorizer, ENGLISH_STOP_WORDS \n",
    "\n",
    "# Define the stop words\n",
    "my_stop_words = ENGLISH_STOP_WORDS.union(['airline', 'airlines', '@', 'am', 'pm'])\n",
    " \n",
    "# Build and fit the vectorizers\n",
    "vect1 = CountVectorizer(stop_words=my_stop_words)\n",
    "vect2 = CountVectorizer(stop_words=ENGLISH_STOP_WORDS)\n",
    "vect1.fit(tweets.text)\n",
    "vect2.fit(tweets.negative_reason)\n",
    "\n",
    "# Print the last 15 features from the first, and all from second vectorizer\n",
    "print(vect1.get_feature_names()[-15:])\n",
    "print(vect2.get_feature_names())\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    ['yesterday', 'yo', 'york', 'youcouldntmakethis', 'yr', 'ywg', 'yxe', 'yyj', 'yyz', 'zambia', 'zcbjyo6lsn', 'zcc82u', \n",
    "    'zero', 'zfqmpgxvs6', 'zone']\n",
    "    \n",
    "    ['attendant', 'bad', 'booking', 'cancelled', 'complaints', 'customer', 'damaged', 'flight', 'issue', 'late', 'longlines',\n",
    "    'lost', 'luggage', 'problems', 'service', 'tell']\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a07d7c",
   "metadata": {},
   "source": [
    "## Capturing a token pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17d8af3",
   "metadata": {},
   "source": [
    "### Specify the token pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3418fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and fit the vectorizer\n",
    "vect = CountVectorizer(token_pattern=r'\\b[^\\d\\W][^\\d\\W]+\\b').fit(tweets.text)\n",
    "vect.transform(tweets.text)\n",
    "print('Length of vectorizer: ', len(vect.get_feature_names()))\n",
    "\n",
    "# Length of vectorizer:  2770"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042aceb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the first vectorizer\n",
    "vect1 = CountVectorizer().fit(tweets.text)\n",
    "vect1.transform(tweets.text)\n",
    "\n",
    "# Build the second vectorizer\n",
    "vect2 = CountVectorizer(token_pattern=r'\\b[^\\d\\W][^\\d\\W]').fit(tweets.text)\n",
    "vect2.transform(tweets.text)\n",
    "\n",
    "# Print out the length of each vectorizer\n",
    "print('Length of vectorizer 1: ', len(vect1.get_feature_names()))\n",
    "print('Length of vectorizer 2: ', len(vect2.get_feature_names()))\n",
    "\n",
    "\"\"\"\n",
    "    Length of vectorizer 1:  3081\n",
    "    Length of vectorizer 2:  332\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9a9744",
   "metadata": {},
   "source": [
    "### String operators with the Twitter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d3607b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the word tokenizing package\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# Tokenize the text column\n",
    "word_tokens = [word_tokenize(review) for review in tweets.text]\n",
    "print('Original tokens: ', word_tokens[0])\n",
    "\n",
    "# Filter out non-letter characters\n",
    "cleaned_tokens = [[word for word in item if word.isalpha()] for item in word_tokens]\n",
    "print('Cleaned tokens: ', cleaned_tokens[0])\n",
    "\n",
    "\"\"\"\n",
    "    Original tokens:  ['@', 'VirginAmerica', 'What', '@', 'dhepburn', 'said', '.']\n",
    "    Cleaned tokens:  ['VirginAmerica', 'What', 'dhepburn', 'said']\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1424d3e",
   "metadata": {},
   "source": [
    "### More string operators and Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2740c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of lists, containing the tokens from list_tweets\n",
    "tokens = [word_tokenize(item) for item in tweets_list]\n",
    "\n",
    "# Remove characters and digits , i.e. retain only letters\n",
    "letters = [[word for word in item if word.isalpha()] for item in tokens]\n",
    "# Remove characters, i.e. retain only letters and digits\n",
    "let_digits = [[word for word in item if word.isalnum()] for item in tokens]\n",
    "# Remove letters and characters, retain only digits\n",
    "digits = [[word for word in item if word.isdigit()] for item in tokens]\n",
    "\n",
    "# Print the last item in each list\n",
    "print('Last item in alphabetic list: ', letters[2])\n",
    "print('Last item in list of alphanumerics: ', let_digits[2])\n",
    "print('Last item in the list of digits: ', digits[2])\n",
    "\n",
    "\"\"\"\n",
    "[\"@VirginAmerica it's really aggressive to blast obnoxious 'entertainment' in your guests' faces &amp; they have little \n",
    "recourse\", \"@VirginAmerica Hey, first time flyer next week - excited! But I'm having a hard time getting my flights added to \n",
    "my Elevate account. Help?\", '@united Change made in just over 3 hours. For something that should have taken seconds online, \n",
    "I am not thrilled. Loved the agent, though.']\n",
    "\n",
    "\n",
    "Last item in alphabetic list:  ['united', 'Change', 'made', 'in', 'just', 'over', 'hours', 'For', 'something', 'that', 'should',\n",
    "'have', 'taken', 'seconds', 'online', 'I', 'am', 'not', 'thrilled', 'Loved', 'the', 'agent', 'though']\n",
    "\n",
    "Last item in list of alphanumerics:  ['united', 'Change', 'made', 'in', 'just', 'over', '3', 'hours', 'For', 'something','that',\n",
    "'should', 'have', 'taken', 'seconds', 'online', 'I', 'am', 'not', 'thrilled', 'Loved', 'the', 'agent', 'though']\n",
    "\n",
    "Last item in the list of digits:  ['3']\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40b2a4b",
   "metadata": {},
   "source": [
    "## Stemming and lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a46c13",
   "metadata": {},
   "source": [
    "### Stems and lemmas from GoT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43897d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required packages from nltk\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "\n",
    "porter = PorterStemmer()\n",
    "WNlemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Tokenize the GoT string\n",
    "tokens = word_tokenize(GoT) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89d35e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Log the start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Build a stemmed list\n",
    "stemmed_tokens = [porter.stem(token) for token in tokens]\n",
    "\n",
    "# Log the end time\n",
    "end_time = time.time()\n",
    "\n",
    "print('Time taken for stemming in seconds: ', end_time - start_time)\n",
    "print('Stemmed tokens: ', stemmed_tokens) \n",
    "\n",
    "\"\"\"\n",
    "Time taken for stemming in seconds:  0.0008411407470703125\n",
    "\n",
    "Stemmed tokens:  ['never', 'forget', 'what', 'you', 'are', ',', 'for', 'sure', 'the', 'world', 'will', 'not', '.', 'make', \n",
    "'it', 'your', 'strength', '.', 'then', 'it', 'can', 'never', 'be', 'your', 'weak', '.', 'armour', 'yourself', 'in', 'it', ',', \n",
    "'and', 'it', 'will', 'never', 'be', 'use', 'to', 'hurt', 'you', '.']\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba775ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Log the start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Build a lemmatized list\n",
    "lem_tokens = [WNlemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "# Log the end time\n",
    "end_time = time.time()\n",
    "\n",
    "print('Time taken for lemmatizing in seconds: ', end_time - start_time)\n",
    "print('Lemmatized tokens: ', lem_tokens) \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Time taken for lemmatizing in seconds:  1.321437120437622\n",
    "\n",
    "Lemmatized tokens:  ['Never', 'forget', 'what', 'you', 'are', ',', 'for', 'surely', 'the', 'world', 'will', 'not', '.', \n",
    "'Make', 'it', 'your', 'strength', '.', 'Then', 'it', 'can', 'never', 'be', 'your', 'weakness', '.', 'Armour', 'yourself',\n",
    "'in', 'it', ',', 'and', 'it', 'will', 'never', 'be', 'used', 'to', 'hurt', 'you', '.']\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a753eb21",
   "metadata": {},
   "source": [
    "### Stem Spanish reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ef5180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the language detection package\n",
    "import langdetect\n",
    "\n",
    "# Loop over the rows of the dataset and append  \n",
    "languages = [] \n",
    "for i in range(len(non_english_reviews)):\n",
    "    languages.append(langdetect.detect_langs(non_english_reviews.iloc[i, 1]))\n",
    "\n",
    "# Clean the list by splitting     \n",
    "languages = [str(lang).split(':')[0][1:] for lang in languages]\n",
    "# Assign the list to a new feature \n",
    "non_english_reviews['language'] = languages\n",
    "\n",
    "# Select the Spanish ones\n",
    "filtered_reviews = non_english_reviews[non_english_reviews.language == 'es']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c56b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required packages\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# Import the Spanish SnowballStemmer\n",
    "SpanishStemmer = SnowballStemmer(\"spanish\")\n",
    "\n",
    "# Create a list of tokens\n",
    "tokens = [word_tokenize(review) for review in filtered_reviews.review] \n",
    "# Stem the list of tokens\n",
    "stemmed_tokens = [[SpanishStemmer.stem(word) for word in token] for token in tokens]\n",
    "\n",
    "# Print the first item of the stemmed tokenss\n",
    "print(stemmed_tokens[0])\n",
    "\n",
    "\"\"\"\n",
    "['la', 'reencarn', 'vist', 'por', 'un', 'cientif', ':', 'el', 'prim', 'libr', 'del', 'dr.', 'weiss', 'sig', 'siend', 'un', \n",
    "'gran', 'libr', 'par', 'tod', 'aquell', 'a', 'quien', 'les', 'inquiet', 'el', 'tem', 'de', 'la', 'reencarn', ',', 'asi', \n",
    "'no', 'cre', 'en', 'ella', '.']\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ade51a",
   "metadata": {},
   "source": [
    "### Stems from tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05d2a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the function to perform stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# Call the stemmer\n",
    "porter = PorterStemmer()\n",
    "\n",
    "# Transform the array of tweets to tokens\n",
    "tokens = [word_tokenize(tweet) for tweet in tweets]\n",
    "# Stem the list of tokens\n",
    "stemmed_tokens = [[porter.stem(word) for word in tweet] for tweet in tokens] \n",
    "# Print the first element of the list\n",
    "print(stemmed_tokens[0])\n",
    "\n",
    "# ['@', 'virginamerica', 'what', '@', 'dhepburn', 'said', '.']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506a5cc3",
   "metadata": {},
   "source": [
    "## TfIdf: More ways to transform text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2db7f8",
   "metadata": {},
   "source": [
    "### Your first TfIdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5f3610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required function\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "annak = ['Happy families are all alike;', 'every unhappy family is unhappy in its own way']\n",
    "\n",
    "# Call the vectorizer and fit it\n",
    "anna_vect = TfidfVectorizer().fit(annak)\n",
    "\n",
    "# Create the tfidf representation\n",
    "anna_tfidf = anna_vect.transform(annak)\n",
    "\n",
    "# Print the result \n",
    "print(anna_tfidf.toarray())\n",
    "\n",
    "\"\"\"\n",
    "    [[0.4472136  0.4472136  0.4472136  0.         0.4472136  0.\n",
    "      0.4472136  0.         0.         0.         0.         0.\n",
    "      0.        ]\n",
    "     [0.         0.         0.         0.30151134 0.         0.30151134\n",
    "      0.         0.30151134 0.30151134 0.30151134 0.30151134 0.60302269\n",
    "      0.30151134]]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140be175",
   "metadata": {},
   "source": [
    "### TfIdf on Twitter airline sentiment data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c594b296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required vectorizer package and stop words list\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS\n",
    "\n",
    "# Define the vectorizer and specify the arguments\n",
    "my_pattern = r'\\b[^\\d\\W][^\\d\\W]+\\b'\n",
    "vect = TfidfVectorizer(ngram_range=(1,2), max_features=100, token_pattern=my_pattern, stop_words=ENGLISH_STOP_WORDS).fit(tweets.text)\n",
    "\n",
    "# Transform the vectorizer\n",
    "X_txt = vect.transform(tweets.text)\n",
    " \n",
    "# Transform to a data frame and specify the column names\n",
    "X=pd.DataFrame(X_txt.toarray(), columns=vect.get_feature_names())\n",
    "print('Top 5 rows of the DataFrame: ', X.head())\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    Top 5 rows of the DataFrame:     agent  airline  airport    amp  austin  ...  wait  way  website  work  yes\n",
    "    0    0.0      0.0      0.0  0.000     0.0  ...   0.0  0.0      0.0   0.0  0.0\n",
    "    1    0.0      0.0      0.0  0.000     0.0  ...   0.0  0.0      0.0   0.0  0.0\n",
    "    2    0.0      0.0      0.0  0.000     0.0  ...   0.0  0.0      0.0   0.0  0.0\n",
    "    3    0.0      0.0      0.0  0.634     0.0  ...   0.0  0.0      0.0   0.0  0.0\n",
    "    4    0.0      0.0      0.0  0.000     0.0  ...   0.0  0.0      0.0   0.0  0.0\n",
    "    \n",
    "    [5 rows x 100 columns]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d160e929",
   "metadata": {},
   "source": [
    "### Tfidf and a BOW on same data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011ffda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required packages\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Build a BOW and tfidf vectorizers from the review column and with max of 100 features\n",
    "vect1 = CountVectorizer(max_features=100).fit(reviews.review)\n",
    "vect2 = TfidfVectorizer(max_features=100).fit(reviews.review)\n",
    "\n",
    "# Transform the vectorizers\n",
    "X1 = vect1.transform(reviews.review)\n",
    "X2 = vect2.transform(reviews.review)\n",
    "# Create DataFrames from the vectorizers\n",
    "X_df1 = pd.DataFrame(X1.toarray(), columns=vect1.get_feature_names())\n",
    "X_df2 = pd.DataFrame(X2.toarray(), columns=vect2.get_feature_names())\n",
    "print('Top 5 rows, using BOW: \\n', X_df1.head())\n",
    "print('Top 5 rows using tfidf: \\n', X_df2.head())\n",
    "\n",
    "\"\"\"\n",
    "    Top 5 rows, using BOW: \n",
    "        about  after  all  also  am  ...  with  work  would  you  your\n",
    "    0      0      0    1     0   0  ...     1     0      2    0     1\n",
    "    1      0      0    0     0   0  ...     0     0      1    1     0\n",
    "    2      0      0    3     0   0  ...     0     1      1    2     0\n",
    "    3      0      0    0     0   0  ...     0     0      0    0     0\n",
    "    4      0      1    0     0   0  ...     0     0      0    3     1\n",
    "    \n",
    "    [5 rows x 100 columns]\n",
    "    \n",
    "    \n",
    "    Top 5 rows using tfidf: \n",
    "        about  after    all  also   am  ...   with   work  would    you   your\n",
    "    0    0.0  0.000  0.139   0.0  0.0  ...  0.113  0.000  0.307  0.000  0.175\n",
    "    1    0.0  0.000  0.000   0.0  0.0  ...  0.000  0.000  0.139  0.106  0.000\n",
    "    2    0.0  0.000  0.285   0.0  0.0  ...  0.000  0.139  0.105  0.160  0.000\n",
    "    3    0.0  0.000  0.000   0.0  0.0  ...  0.000  0.000  0.000  0.000  0.000\n",
    "    4    0.0  0.174  0.000   0.0  0.0  ...  0.000  0.000  0.000  0.328  0.163\n",
    "    \n",
    "    [5 rows x 100 columns]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2489851d",
   "metadata": {},
   "source": [
    "# 4. Let's Predict the Sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3dd221c",
   "metadata": {},
   "source": [
    "## Let's Predict the Sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae16724",
   "metadata": {},
   "source": [
    "### Logistic regression of movie reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880cb870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Define the vector of targets and matrix of features\n",
    "y = movies.label\n",
    "X = movies.drop('label', axis=1)\n",
    "\n",
    "# Build a logistic regression model and calculate the accuracy\n",
    "log_reg = LogisticRegression().fit(X, y)\n",
    "print('Accuracy of logistic regression: ', log_reg.score(X, y))\n",
    "\n",
    "# Accuracy of logistic regression:  0.7852286361818425"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04d00ab",
   "metadata": {},
   "source": [
    "### Logistic regression using Twitter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdf0771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the vector of targets and matrix of features\n",
    "y = tweets.airline_sentiment\n",
    "X = tweets.drop('airline_sentiment', axis=1)\n",
    "\n",
    "# Build a logistic regression model and calculate the accuracy\n",
    "log_reg = LogisticRegression().fit(X, y)\n",
    "print('Accuracy of logistic regression: ', log_reg.score(X, y))\n",
    "\n",
    "# Create an array of prediction\n",
    "y_predict = log_reg.predict(X)\n",
    "\n",
    "# Print the accuracy using accuracy score\n",
    "print('Accuracy of logistic regression: ', accuracy_score(y, y_predict))\n",
    "\n",
    "\"\"\"\n",
    "    Accuracy of logistic regression:  0.8555327868852459\n",
    "    Accuracy of logistic regression:  0.8555327868852459\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a524abc",
   "metadata": {},
   "source": [
    "## Did we really predict the sentiment well?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad775c1",
   "metadata": {},
   "source": [
    "### Build and assess a model: movies reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc53b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required packages\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the vector of labels and matrix of features\n",
    "y = movies.label\n",
    "X = movies.drop('label', axis=1)\n",
    "\n",
    "# Perform the train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build a logistic regression model and print out the accuracy\n",
    "log_reg = LogisticRegression().fit(X_train, y_train)\n",
    "print('Accuracy on train set: ', log_reg.score(X_train, y_train))\n",
    "print('Accuracy on test set: ', log_reg.score(X_test, y_test))\n",
    "\n",
    "\"\"\"\n",
    "Accuracy on train set:  0.7861666666666667\n",
    "Accuracy on test set:  0.7521652231845436\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05475f44",
   "metadata": {},
   "source": [
    "### Performance metrics of Twitter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff32372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=123, stratify=y)\n",
    "\n",
    "# Train a logistic regression\n",
    "log_reg = LogisticRegression().fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_predicted = log_reg.predict(X_test)\n",
    " \n",
    "# Print the performance metrics\n",
    "print('Accuracy score test set: ', accuracy_score(y_test, y_predicted))\n",
    "print('Confusion matrix test set: \\n', confusion_matrix(y_test, y_predicted)/len(y_test))\n",
    "\n",
    "\"\"\"\n",
    "Accuracy score test set:  0.8031854379977247\n",
    "Confusion matrix test set: \n",
    "     [[0.57337884 0.05346985 0.00568828]\n",
    "     [0.04209329 0.13879408 0.02730375]\n",
    "     [0.01934016 0.04891923 0.09101251]]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3513ea",
   "metadata": {},
   "source": [
    "### Build and assess a model: product reviews data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9270a825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the accuracy and confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# Split the data into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Build a logistic regression\n",
    "log_reg = LogisticRegression().fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels \n",
    "y_predict = log_reg.predict(X_test)\n",
    "\n",
    "# Print the performance metrics\n",
    "print('Accuracy score of test data: ', accuracy_score(y_test, y_predict))\n",
    "print('Confusion matrix of test data: \\n', confusion_matrix(y_test, y_predict)/len(y_test))\n",
    "\n",
    "\"\"\"\n",
    "Accuracy score of test data:  0.7853333333333333\n",
    "Confusion matrix of test data: \n",
    "     [[0.39333333 0.11266667]\n",
    "     [0.102      0.392     ]]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ea522c",
   "metadata": {},
   "source": [
    "## Logistic regression: revisited"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c393e7",
   "metadata": {},
   "source": [
    "### Predict probabilities of movie reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f804f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=321)\n",
    "\n",
    "# Train a logistic regression\n",
    "log_reg = LogisticRegression().fit(X_train, y_train)\n",
    "\n",
    "# Predict the probability of the 0 class\n",
    "prob_0 = log_reg.predict_proba(X_test)[:, 0]\n",
    "# Predict the probability of the 1 class\n",
    "prob_1 = log_reg.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"First 10 predicted probabilities of class 0: \", prob_0[:10])\n",
    "print(\"First 10 predicted probabilities of class 1: \", prob_1[:10])\n",
    "\n",
    "\"\"\"\n",
    "First 10 predicted probabilities of class 0:  [0.86210184 0.90317521 0.60800676 0.15831127 0.86473322 0.87870788\n",
    "     0.61080321 0.78899465 0.4451038  0.3082362 ]\n",
    "     \n",
    "First 10 predicted probabilities of class 1:  [0.13789816 0.09682479 0.39199324 0.84168873 0.13526678 0.12129212\n",
    "     0.38919679 0.21100535 0.5548962  0.6917638 ]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da090727",
   "metadata": {},
   "source": [
    "### Product reviews with regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f0436b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# Train a logistic regression with regularization of 1000\n",
    "log_reg1 = LogisticRegression(C=1000).fit(X_train, y_train)\n",
    "# Train a logistic regression with regularization of 0.001\n",
    "log_reg2 = LogisticRegression(C=0.001).fit(X_train, y_train)\n",
    "\n",
    "# Print the accuracies\n",
    "print('Accuracy of model 1: ', log_reg1.score(X_test, y_test))\n",
    "print('Accuracy of model 2: ', log_reg2.score(X_test, y_test))\n",
    "\n",
    "\"\"\"\n",
    "    Accuracy of model 1:  0.786\n",
    "    Accuracy of model 2:  0.7405\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0729247f",
   "metadata": {},
   "source": [
    "### Regularizing models with Twitter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc19f5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a logistic regression with regularizarion parameter of 100\n",
    "log_reg1 = LogisticRegression(C=100).fit(X_train, y_train)\n",
    "# Build a logistic regression with regularizarion parameter of 0.1\n",
    "log_reg2 = LogisticRegression(C=0.1).fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for each model\n",
    "y_predict1 = log_reg1.predict(X_test)\n",
    "y_predict2 = log_reg2.predict(X_test)\n",
    "\n",
    "# Print performance metrics for each model\n",
    "print('Accuracy of model 1: ', accuracy_score(y_test, y_predict1))\n",
    "print('Accuracy of model 2: ', accuracy_score(y_test, y_predict2))\n",
    "print('Confusion matrix of model 1: \\n', confusion_matrix(y_test, y_predict1)/len(y_test))\n",
    "print('Confusion matrix of model 2: \\n', confusion_matrix(y_test, y_predict2)/len(y_test))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Accuracy of model 1:  0.8156996587030717\n",
    "\n",
    "Accuracy of model 2:  0.8156996587030717\n",
    "\n",
    "Confusion matrix of model 1: \n",
    "     [[0.56484642 0.05290102 0.01023891]\n",
    "     [0.02559727 0.16040956 0.02047782]\n",
    "     [0.0221843  0.05290102 0.09044369]]\n",
    "     \n",
    "Confusion matrix of model 2: \n",
    "     [[0.58361775 0.04266212 0.00170648]\n",
    "     [0.0443686  0.14675768 0.01535836]\n",
    "     [0.02730375 0.05290102 0.08532423]]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babe3e59",
   "metadata": {},
   "source": [
    "## Bringing it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51631936",
   "metadata": {},
   "source": [
    "### Step 1: Word cloud and feature creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43deade5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and generate a word cloud image\n",
    "cloud_positives = WordCloud(background_color='white').generate(positive_reviews)\n",
    " \n",
    "# Display the generated wordcloud image\n",
    "plt.imshow(cloud_positives, interpolation='bilinear') \n",
    "plt.axis(\"off\")\n",
    "\n",
    "# Don't forget to show the final image\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975f002a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize each item in the review column\n",
    "word_tokens = [word_tokenize(review) for review in reviews.review]\n",
    "\n",
    "# Create an empty list to store the length of the reviews\n",
    "len_tokens = []\n",
    "\n",
    "# Iterate over the word_tokens list and determine the length of each item\n",
    "for i in range(len(word_tokens)):\n",
    "     len_tokens.append(len(word_tokens[i]))\n",
    "\n",
    "# Create a new feature for the lengh of each review\n",
    "reviews['n_words'] = len_tokens "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591356dc",
   "metadata": {},
   "source": [
    "### Step 2: Building a vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699c024f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the TfidfVectorizer and default list of English stop words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS \n",
    "\n",
    "# Build the vectorizer\n",
    "vect = TfidfVectorizer(stop_words=ENGLISH_STOP_WORDS, ngram_range=(1, 2), max_features=200, token_pattern=r'\\b[^\\d\\W][^\\d\\W]+\\b').fit(reviews.review)\n",
    "# Create sparse matrix from the vectorizer\n",
    "X = vect.transform(reviews.review)\n",
    "\n",
    "# Create a DataFrame\n",
    "reviews_transformed = pd.DataFrame(X.toarray(), columns=vect.get_feature_names())\n",
    "print('Top 5 rows of the DataFrame: \\n', reviews_transformed.head())\n",
    "\n",
    "\"\"\"\n",
    "    Top 5 rows of the DataFrame: \n",
    "        able  action  actually  ago  album  ...  writing  written  wrong  year  years\n",
    "    0   0.0     0.0       0.0  0.0    0.0  ...      0.0      0.0    0.0   0.0  0.000\n",
    "    1   0.0     0.0       0.0  0.0    0.0  ...      0.0      0.0    0.0   0.0  0.209\n",
    "    2   0.0     0.0       0.0  0.0    0.0  ...      0.0      0.0    0.0   0.0  0.152\n",
    "    3   0.0     0.0       0.0  0.0    0.0  ...      0.0      0.0    0.0   0.0  0.000\n",
    "    4   0.0     0.0       0.0  0.0    0.0  ...      0.0      0.0    0.0   0.0  0.000\n",
    "    \n",
    "    [5 rows x 200 columns]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244f0a3d",
   "metadata": {},
   "source": [
    "### Step 3: Building a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66178fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define X and y\n",
    "y = reviews_transformed.score\n",
    "X = reviews_transformed.drop('score', axis=1)\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=456)\n",
    "\n",
    "# Train a logistic regression\n",
    "log_reg = LogisticRegression().fit(X_train, y_train)\n",
    "# Predict the labels\n",
    "y_predicted = log_reg.predict(X_test)\n",
    "\n",
    "# Print accuracy score and confusion matrix on test set\n",
    "print('Accuracy on the test set: ', accuracy_score(y_test, y_predicted))\n",
    "print(confusion_matrix(y_test, y_predicted)/len(y_test))\n",
    "\n",
    "\"\"\"\n",
    "Accuracy on the test set:  0.787\n",
    "[[0.4115 0.1145]\n",
    "[0.0985 0.3755]]\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
