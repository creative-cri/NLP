{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1858853",
   "metadata": {},
   "source": [
    "# Feature Engineering for NLP in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e81a407",
   "metadata": {},
   "source": [
    "# 1. Basic features and readability scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f72da5",
   "metadata": {},
   "source": [
    "## Introduction to NLP feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d284c62",
   "metadata": {},
   "source": [
    "### One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aab7178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the features of df1\n",
    "print(df1.columns)\n",
    "\n",
    "\"\"\"\n",
    "Index(['feature 1', 'feature 2', 'feature 3', 'feature 4', 'feature 5', 'label'], dtype='object')\n",
    "\"\"\"\n",
    "\n",
    "# Perform one-hot encoding\n",
    "df1 = pd.get_dummies(df1, columns=['feature 5'])\n",
    "\n",
    "# Print the new features of df1\n",
    "print(df1.columns)\n",
    "\n",
    "\"\"\"\n",
    "Index(['feature 1', 'feature 2', 'feature 3', 'feature 4', 'label', 'feature 5_female', 'feature 5_male'], dtype='object')\n",
    "\"\"\"\n",
    "\n",
    "# Print first five rows of df1\n",
    "print(df1.head())\n",
    "\n",
    "\"\"\"\n",
    "       feature 1  feature 2  feature 3  feature 4  label  feature 5_female  feature 5_male\n",
    "    0     29.000          0          0    211.338      1                 1               0\n",
    "    1      0.917          1          2    151.550      1                 0               1\n",
    "    2      2.000          1          2    151.550      0                 1               0\n",
    "    3     30.000          1          2    151.550      0                 0               1\n",
    "    4     25.000          1          2    151.550      0                 1               0\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8730d346",
   "metadata": {},
   "source": [
    "## Basic feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f136203c",
   "metadata": {},
   "source": [
    "### Character count of Russian tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ee0e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a feature char_count\n",
    "tweets['char_count'] = tweets['content'].apply(len)\n",
    "\n",
    "# Print the average character count\n",
    "print(tweets['char_count'].mean()\n",
    "      \n",
    "# 103.462"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97910cdd",
   "metadata": {},
   "source": [
    "### Word count of TED talks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fba9dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that returns number of words in a string\n",
    "def count_words(string):\n",
    "\t# Split the string into words\n",
    "    words = string.split()\n",
    "    \n",
    "    # Return the number of words\n",
    "    return len(words)\n",
    "\n",
    "# Create a new feature word_count\n",
    "ted['word_count'] = ted['transcript'].apply(count_words)\n",
    "\n",
    "# Print the average word count of the talks\n",
    "print(ted['word_count'].mean())\n",
    "\n",
    "# 1987.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb612fd",
   "metadata": {},
   "source": [
    "### Hashtags and mentions in Russian tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce39a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that returns number of hashtags in a string\n",
    "def count_hashtags(string):\n",
    "\t# Split the string into words\n",
    "    words = string.split()\n",
    "    \n",
    "    # Create a list of words that are hashtags\n",
    "    hashtags = [word for word in words if word.startswith('#')]\n",
    "    \n",
    "    # Return number of hashtags\n",
    "    return(len(hashtags))\n",
    "\n",
    "# Create a feature hashtag_count and display distribution\n",
    "tweets['hashtag_count'] = tweets['content'].apply(count_hashtags)\n",
    "tweets['hashtag_count'].hist()\n",
    "plt.title('Hashtag count distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596eabc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that returns number of mentions in a string\n",
    "def count_mentions(string):\n",
    "\t# Split the string into words\n",
    "    words = string.split()\n",
    "    \n",
    "    # Create a list of words that are mentions\n",
    "    mentions = [word for word in words if word.startswith('@')]\n",
    "    \n",
    "    # Return number of mentions\n",
    "    return(len(mentions))\n",
    "\n",
    "# Create a feature mention_count and display distribution\n",
    "tweets['mention_count'] = tweets['content'].apply(count_mentions)\n",
    "tweets['mention_count'].hist()\n",
    "plt.title('Mention count distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2becedc5",
   "metadata": {},
   "source": [
    "## Readability tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10acde40",
   "metadata": {},
   "source": [
    "### Readability of 'The Myth of Sisyphus'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec7aec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Textatistic\n",
    "from textatistic import Textatistic\n",
    "\n",
    "# Compute the readability scores \n",
    "readability_scores = Textatistic(sisyphus_essay).scores\n",
    "\n",
    "# Print the flesch reading ease score\n",
    "flesch = readability_scores['flesch_score']\n",
    "print(\"The Flesch Reading Ease is %.2f\" % (flesch))\n",
    "\n",
    "# The Flesch Reading Ease is 81.67"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89dc37d",
   "metadata": {},
   "source": [
    "### Readability of various publications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda471d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Textatistic\n",
    "from textatistic import Textatistic\n",
    "\n",
    "# List of excerpts\n",
    "excerpts = [forbes, harvard_law, r_digest, time_kids]\n",
    "\n",
    "# Loop through excerpts and compute gunning fog index\n",
    "gunning_fog_scores = []\n",
    "for excerpt in excerpts:\n",
    "  readability_scores = Textatistic(excerpt).scores\n",
    "  gunning_fog = readability_scores['gunningfog_score']\n",
    "  gunning_fog_scores.append(gunning_fog)\n",
    "  \n",
    "# Print the gunning fog indices\n",
    "print(gunning_fog_scores)\n",
    "\n",
    "# [14.436002482929858, 20.735401069518716, 11.085587583148559, 5.926785009861934]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f86c289",
   "metadata": {},
   "source": [
    "# 2. Text preprocessing, POS tagging and NER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39315f35",
   "metadata": {},
   "source": [
    "## Tokenization and Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4957212d",
   "metadata": {},
   "source": [
    "### Tokenizing the Gettysburg address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d6a3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the en_core_web_sm model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Create a Doc object\n",
    "doc = nlp(gettysburg)\n",
    "\n",
    "# Generate the tokens\n",
    "tokens = [token.text for token in doc]\n",
    "print(tokens)\n",
    "\n",
    "\"\"\"\n",
    "['Four', 'score', 'and', 'seven', 'years', 'ago', 'our', 'fathers', 'brought', 'forth', 'on', 'this', 'continent', ',', 'a', \n",
    "'new', 'nation', ',', 'conceived', 'in', 'Liberty', ',', 'and', 'dedicated', 'to', 'the', 'proposition', 'that', 'all', 'men',\n",
    "'are', 'created', 'equal', '.', 'Now', 'we', \"'re\", 'engaged', 'in', 'a', 'great', 'civil', 'war', ',', 'testing', 'whether', \n",
    "'that', 'nation', ',', 'or', 'any', 'nation', 'so', 'conceived', 'and', 'so', 'dedicated', ',', 'can', 'long', 'endure', '.', \n",
    "'We', \"'re\", 'met', 'on', 'a', 'great', 'battlefield', 'of', 'that', 'war', '.', 'We', \"'ve\", 'come', 'to', 'dedicate', 'a', \n",
    "'portion', 'of', 'that', 'field', ',', 'as', 'a', 'final', 'resting', 'place', 'for', 'those', 'who', 'here', 'gave', 'their', \n",
    "'lives', 'that', 'that', 'nation', 'might', 'live', '.', 'It', \"'s\", 'altogether', 'fitting', 'and', 'proper', 'that', 'we', \n",
    "'should', 'do', 'this', '.', 'But', ',', 'in', 'a', 'larger', 'sense', ',', 'we', 'ca', \"n't\", 'dedicate', '-', 'we', 'can', \n",
    "'not', 'consecrate', '-', 'we', 'can', 'not', 'hallow', '-', 'this', 'ground', '.', 'The', 'brave', 'men', ',', 'living', 'and',\n",
    "'dead', ',', 'who', 'struggled', 'here', ',', 'have', 'consecrated', 'it', ',', 'far', 'above', 'our', 'poor', 'power', 'to', \n",
    "'add', 'or', 'detract', '.', 'The', 'world', 'will', 'little', 'note', ',', 'nor', 'long', 'remember', 'what', 'we', 'say', \n",
    "'here', ',', 'but', 'it', 'can', 'never', 'forget', 'what', 'they', 'did', 'here', '.', 'It', 'is', 'for', 'us', 'the', \n",
    "'living', ',', 'rather', ',', 'to', 'be', 'dedicated', 'here', 'to', 'the', 'unfinished', 'work', 'which', 'they', 'who', \n",
    "'fought', 'here', 'have', 'thus', 'far', 'so', 'nobly', 'advanced', '.', 'It', \"'s\", 'rather', 'for', 'us', 'to', 'be', 'here',\n",
    "'dedicated', 'to', 'the', 'great', 'task', 'remaining', 'before', 'us', '-', 'that', 'from', 'these', 'honored', 'dead', 'we', \n",
    "'take', 'increased', 'devotion', 'to', 'that', 'cause', 'for', 'which', 'they', 'gave', 'the', 'last', 'full', 'measure', 'of',\n",
    "'devotion', '-', 'that', 'we', 'here', 'highly', 'resolve', 'that', 'these', 'dead', 'shall', 'not', 'have', 'died', 'in', \n",
    "'vain', '-', 'that', 'this', 'nation', ',', 'under', 'God', ',', 'shall', 'have', 'a', 'new', 'birth', 'of', 'freedom', '-', \n",
    "'and', 'that', 'government', 'of', 'the', 'people', ',', 'by', 'the', 'people', ',', 'for', 'the', 'people', ',', 'shall', \n",
    "'not', 'perish', 'from', 'the', 'earth', '.']\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafc387f",
   "metadata": {},
   "source": [
    "### Lemmatizing the Gettysburg address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775bb1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the gettysburg address\n",
    "print(gettysburg)\n",
    "\n",
    "\"\"\"\n",
    "Four score and seven years ago our fathers brought forth on this continent, a new nation, conceived in Liberty, \n",
    "and dedicated to the proposition that all men are created equal. Now we're engaged in a great civil war, testing whether \n",
    "that nation, or any nation so conceived and so dedicated, can long endure. We're met on a great battlefield of that war. \n",
    "We've come to dedicate a portion of that field, as a final resting place for those who here gave their lives that that nation\n",
    "might live. It's altogether fitting and proper that we should do this. But, in a larger sense, we can't dedicate - we can not \n",
    "consecrate - we can not hallow - this ground. The brave men, living and dead, who struggled here, have consecrated it, far above\n",
    "our poor power to add or detract. The world will little note, nor long remember what we say here, but it can never forget what \n",
    "they did here. It is for us the living, rather, to be dedicated here to the unfinished work which they who fought here have \n",
    "thus far so nobly advanced. It's rather for us to be here dedicated to the great task remaining before us - that from these \n",
    "honored dead we take increased devotion to that cause for which they gave the last full measure of devotion - that we here \n",
    "highly resolve that these dead shall not have died in vain - that this nation, under God, shall have a new birth of \n",
    "freedom - and that government of the people, by the people, for the people, shall not perish from the earth. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b55e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the en_core_web_sm model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Create a Doc object\n",
    "doc = nlp(gettysburg)\n",
    "\n",
    "# Generate lemmas\n",
    "lemmas = [token.lemma_ for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd16c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the en_core_web_sm model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Create a Doc object\n",
    "doc = nlp(gettysburg)\n",
    "\n",
    "# Generate lemmas\n",
    "lemmas = [token.lemma_ for token in doc]\n",
    "\n",
    "# Convert lemmas into a string\n",
    "print(' '.join(lemmas))\n",
    "\n",
    "\"\"\"\n",
    " four score and seven year ago our father bring forth on this continent , a new nation , conceive in Liberty , and dedicate to\n",
    " the proposition that all man be create equal . now we be engage in a great civil war , test whether that nation , or any nation\n",
    " so conceive and so dedicated , can long endure . we be meet on a great battlefield of that war . we 've come to dedicate a \n",
    " portion of that field , as a final resting place for those who here give their life that that nation might live . it be \n",
    " altogether fitting and proper that we should do this . but , in a large sense , we ca n't dedicate - we can not consecrate - \n",
    " we can not hallow - this ground . the brave man , live and dead , who struggle here , have consecrate it , far above our poor \n",
    " power to add or detract . the world will little note , nor long remember what we say here , but it can never forget what they \n",
    " do here . it be for we the living , rather , to be dedicate here to the unfinished work which they who fight here have thus \n",
    " far so nobly advanced . it be rather for we to be here dedicate to the great task remain before we - that from these honor \n",
    " dead we take increased devotion to that cause for which they give the last full measure of devotion - that we here highly \n",
    " resolve that these dead shall not have die in vain - that this nation , under God , shall have a new birth of freedom - and \n",
    " that government of the people , by the people , for the people , shall not perish from the earth \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07defef0",
   "metadata": {},
   "source": [
    "## Text cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2444c82a",
   "metadata": {},
   "source": [
    "### Cleaning a blog post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ffd20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and create Doc object\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(blog)\n",
    "\n",
    "# Generate lemmatized tokens\n",
    "lemmas = [token.lemma_ for token in doc]\n",
    "\n",
    "# Remove stopwords and non-alphabetic tokens\n",
    "a_lemmas = [lemma for lemma in lemmas \n",
    "            if lemma.isalpha() and lemma not in stopwords]\n",
    "\n",
    "# Print string after text cleaning\n",
    "print(' '.join(a_lemmas))\n",
    "\n",
    "\"\"\"\n",
    "Twenty-first-century politics has witnessed an alarming rise of populism in the U.S. and Europe. The first warning signs came \n",
    "with the UK Brexit Referendum vote in 2016 swinging in the way of Leave. This was followed by a stupendous victory by \n",
    "billionaire Donald Trump to become the 45th President of the United States in November 2016. Since then, Europe has seen a \n",
    "steady rise in populist and far-right parties that have capitalized on Europe’s Immigration Crisis to raise nationalist and \n",
    "anti-Europe sentiments. Some instances include Alternative for Germany (AfD) winning 12.6% of all seats and entering the \n",
    "Bundestag, thus upsetting Germany’s political order for the first time since the Second World War, the success of the Five Star\n",
    "Movement in Italy and the surge in popularity of neo-nazism and neo-fascism in countries such as Hungary, Czech Republic, Poland\n",
    "and Austria.\n",
    "\n",
    "\n",
    "century politic witness alarming rise populism Europe warning sign come UK Brexit Referendum vote swinging way Leave follow \n",
    "stupendous victory billionaire Donald Trump President United States November Europe steady rise populist far right party \n",
    "capitalize Europe Immigration Crisis raise nationalist anti europe sentiment instance include Alternative Germany AfD win seat \n",
    "enter Bundestag upset Germany political order time Second World War success Five Star Movement Italy surge popularity neo \n",
    "nazism neo fascism country Hungary Czech Republic Poland Austria\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f41ee0",
   "metadata": {},
   "source": [
    "### Cleaning TED talks in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c479dcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess text\n",
    "def preprocess(text):\n",
    "  \t# Create Doc object\n",
    "    doc = nlp(text, disable=['ner', 'parser'])\n",
    "    # Generate lemmas\n",
    "    lemmas = [token.lemma_ for token in doc]\n",
    "    # Remove stopwords and non-alphabetic characters\n",
    "    a_lemmas = [lemma for lemma in lemmas \n",
    "            if lemma.isalpha() and lemma not in stopwords]\n",
    "    \n",
    "    return ' '.join(a_lemmas)\n",
    "  \n",
    "# Apply preprocess to ted['transcript']\n",
    "ted['transcript'] = ted['transcript'].apply(preprocess)\n",
    "print(ted['transcript'])\n",
    "\n",
    "\"\"\"\n",
    "    0     talk new lecture TED I illusion create TED I t...\n",
    "    1     representation brain brain break left half log...\n",
    "    2     great honor today share Digital Universe creat...\n",
    "    3     passion music technology thing combination thi...\n",
    "    4     use want computer new program programming requ...\n",
    "    5     I neuroscientist mixed background physics medi...\n",
    "    6     Pat Mitchell day January begin like work love ...\n",
    "    7     Taylor Wilson I year old I nuclear physicist l...\n",
    "    8     I grow Northern Ireland right north end absolu...\n",
    "    9     I publish article New York Times Modern Love c...\n",
    "    10    Joseph Member Parliament Kenya picture Maasai ...\n",
    "    11    hi I talk little bit music machine life specif...\n",
    "    12    hi let I ask audience question lie child raise...\n",
    "    13    historical record allow know ancient Greeks dr...\n",
    "    14    good morning I little boy I experience change ...\n",
    "    15    I slide I year ago time I short slide morning ...\n",
    "    16    I like world I like share year old love story ...\n",
    "    17    I fail woman I fail feminist I passionate opin...\n",
    "    18    revolution century significant longevity revol...\n",
    "    19    today baffle lady observe shell soul dwellsand...\n",
    "    Name: transcript, dtype: object\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba507f16",
   "metadata": {},
   "source": [
    "## Part-of-speech tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c2e2f9",
   "metadata": {},
   "source": [
    "### POS tagging in Lord of the Flies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab3a937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the en_core_web_sm model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Create a Doc object\n",
    "doc = nlp(lotf)\n",
    "\n",
    "# Generate tokens and pos tags\n",
    "pos = [(token.text, token.pos_) for token in doc]\n",
    "print(pos)\n",
    "\n",
    "\"\"\"\n",
    "He found himself understanding the wearisomeness of this life, where every path was an improvisation and a considerable part of\n",
    "one’s waking life was spent watching one’s feet.\n",
    "\n",
    "\n",
    "[('He', 'PRON'), ('found', 'VERB'), ('himself', 'PRON'), ('understanding', 'VERB'), ('the', 'DET'), ('wearisomeness', 'NOUN'),\n",
    "('of', 'ADP'), ('this', 'DET'), ('life', 'NOUN'), (',', 'PUNCT'), ('where', 'ADV'), ('every', 'DET'), ('path', 'NOUN'), \n",
    "('was', 'VERB'), ('an', 'DET'), ('improvisation', 'NOUN'), ('and', 'CCONJ'), ('a', 'DET'), ('considerable', 'ADJ'), \n",
    "('part', 'NOUN'), ('of', 'ADP'), ('one', 'PRON'), ('’s', 'ADV'), ('waking', 'VERB'), ('life', 'NOUN'), ('was', 'AUX'), \n",
    "('spent', 'VERB'), ('watching', 'VERB'), ('one', 'NUM'), ('’s', 'NOUN'), ('feet', 'NOUN'), ('.', 'PUNCT')]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea62121",
   "metadata": {},
   "source": [
    "### Counting nouns in a piece of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813dedae",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Returns number of proper nouns\n",
    "def proper_nouns(text, model=nlp):\n",
    "  \t# Create doc object\n",
    "    doc = model(text)\n",
    "    # Generate list of POS tags\n",
    "    pos = [token.pos_ for token in doc]\n",
    "    \n",
    "    # Return number of proper nouns\n",
    "    return pos.count('PROPN')\n",
    "\n",
    "print(proper_nouns(\"Abdul, Bill and Cathy went to the market to buy apples.\", nlp))\n",
    "\n",
    "#  3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8b10ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Returns number of other nouns\n",
    "def nouns(text, model=nlp):\n",
    "  \t# Create doc object\n",
    "    doc = model(text)\n",
    "    # Generate list of POS tags\n",
    "    pos = [token.pos_ for token in doc]\n",
    "    \n",
    "    # Return number of other nouns\n",
    "    return pos.count('NOUN')\n",
    "\n",
    "print(nouns(\"Abdul, Bill and Cathy went to the market to buy apples.\", nlp))\n",
    "\n",
    "# 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f811a33",
   "metadata": {},
   "source": [
    "### Noun usage in fake news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0d8490",
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines['num_propn'] = headlines['title'].apply(proper_nouns)\n",
    "\n",
    "# Compute mean of proper nouns\n",
    "real_propn = headlines[headlines['label'] == 'REAL']['num_propn'].mean()\n",
    "fake_propn = headlines[headlines['label'] == 'FAKE']['num_propn'].mean()\n",
    "\n",
    "# Print results\n",
    "print(\"Mean no. of proper nouns in real and fake headlines are %.2f and %.2f respectively\"%(real_propn, fake_propn))\n",
    "\n",
    "# Mean no. of proper nouns in real and fake headlines are 2.40 and 4.67 respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12665dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines['num_noun'] = headlines['title'].apply(nouns)\n",
    "\n",
    "# Compute mean of other nouns\n",
    "real_noun = headlines[headlines['label'] == 'REAL']['num_noun'].mean()\n",
    "fake_noun = headlines[headlines['label'] == 'FAKE']['num_noun'].mean()\n",
    "\n",
    "# Print results\n",
    "print(\"Mean no. of other nouns in real and fake headlines are %.2f and %.2f respectively\"%(real_noun, fake_noun))\n",
    "\n",
    "\n",
    "# Mean no. of other nouns in real and fake headlines are 2.28 and 1.84 respectively"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1614b2",
   "metadata": {},
   "source": [
    "## Named entity recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd81926",
   "metadata": {},
   "source": [
    "### Named entities in a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb60968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the required model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Create a Doc instance \n",
    "text = 'Sundar Pichai is the CEO of Google. Its headquarters is in Mountain View.'\n",
    "doc = nlp(text)\n",
    "\n",
    "# Print all named entities and their labels\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n",
    "    \n",
    "\"\"\"\n",
    "    Google ORG\n",
    "    Mountain View GPE\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f9f048",
   "metadata": {},
   "source": [
    "### Identifying people mentioned in a news article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abb00b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_persons(text):\n",
    "  # Create Doc object\n",
    "  doc = nlp(text)\n",
    "  \n",
    "  # Identify the persons\n",
    "  persons = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\n",
    "  \n",
    "  # Return persons\n",
    "  return persons\n",
    "\n",
    "print(find_persons(tc))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "It’s' been a busy day for Facebook  exec op-eds. Earlier this morning, Sheryl Sandberg broke the site’s silence around the \n",
    "Christchurch massacre, and now Mark Zuckerberg is calling on governments and other bodies to increase regulation around the \n",
    "sorts of data Facebook traffics in. He’s hoping to get out in front of heavy-handed regulation and get a seat at the table \n",
    "shaping it.\n",
    "\n",
    "\n",
    "['Sheryl Sandberg', 'Mark Zuckerberg']\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362b45d1",
   "metadata": {},
   "source": [
    "# 3. N-Gram models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294743a9",
   "metadata": {},
   "source": [
    "## Building a bag of words model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d5266b",
   "metadata": {},
   "source": [
    "### BoW model for movie taglines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0333cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create CountVectorizer object\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Generate matrix of word vectors\n",
    "bow_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Print the shape of bow_matrix\n",
    "print(bow_matrix.shape)\n",
    "\n",
    "\"\"\"\n",
    "1            Roll the dice and unleash the excitement!\n",
    "2    Still Yelling. Still Fighting. Still Ready for...\n",
    "3    Friends are the people who let you be yourself...\n",
    "4    Just When His World Is Back To Normal... He's ...\n",
    "5                             A Los Angeles Crime Saga\n",
    "Name: tagline, dtype: object\n",
    "\n",
    "<script.py> output:\n",
    "    (7033, 6614)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d096ac",
   "metadata": {},
   "source": [
    "### Analyzing dimensionality and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765b91c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create CountVectorizer object\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Generate matrix of word vectors\n",
    "bow_lem_matrix = vectorizer.fit_transform(lem_corpus)\n",
    "\n",
    "# Print the shape of bow_lem_matrix\n",
    "print(bow_lem_matrix.shape)\n",
    "\n",
    "\"\"\"\n",
    "0    roll dice unleash excitement\n",
    "1           yell fight ready love\n",
    "2    friend people let let forget\n",
    "3      world normal surprise life\n",
    "4          los angeles crime saga\n",
    "Name: 1, dtype: object\n",
    "\n",
    "<script.py> output:\n",
    "    (6959, 5223)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3c0885",
   "metadata": {},
   "source": [
    "### Mapping feature indices with feature names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba9fd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CountVectorizer object\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Generate matrix of word vectors\n",
    "bow_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Convert bow_matrix into a DataFrame\n",
    "bow_df = pd.DataFrame(bow_matrix.toarray())\n",
    "\n",
    "# Map the column names to vocabulary \n",
    "bow_df.columns = vectorizer.get_feature_names()\n",
    "\n",
    "# Print bow_df\n",
    "print(bow_df)\n",
    "\n",
    "\"\"\"\n",
    "['The lion is the king of the jungle', 'Lions have lifespans of a decade', 'The lion is an endangered species']\n",
    "\n",
    "<script.py> output:\n",
    "       an  decade  endangered  have  is  ...  lion  lions  of  species  the\n",
    "    0   0       0           0     0   1  ...     1      0   1        0    3\n",
    "    1   0       1           0     1   0  ...     0      1   1        0    0\n",
    "    2   1       0           1     0   1  ...     1      0   0        1    1\n",
    "    \n",
    "    [3 rows x 13 columns]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fbb43a",
   "metadata": {},
   "source": [
    "## Building a BoW Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb6ce25",
   "metadata": {},
   "source": [
    "### BoW vectors for movie reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8f6776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create a CountVectorizer object\n",
    "vectorizer = CountVectorizer(lowercase=True, stop_words='english')\n",
    "\n",
    "# Fit and transform X_train\n",
    "X_train_bow = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform X_test\n",
    "X_test_bow = vectorizer.transform(X_test)\n",
    "\n",
    "# Print shape of X_train_bow and X_test_bow\n",
    "print(X_train_bow.shape)\n",
    "print(X_test_bow.shape)\n",
    "\n",
    "\"\"\"\n",
    "    (250, 8158)\n",
    "    (750, 8158)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae7194a",
   "metadata": {},
   "source": [
    "### Predicting the sentiment of a movie review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041eff58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a MultinomialNB object\n",
    "clf = MultinomialNB()\n",
    "\n",
    "# Fit the classifier\n",
    "clf.fit(X_train_bow, y_train)\n",
    "\n",
    "# Measure the accuracy\n",
    "accuracy = clf.score(X_test_bow, y_test)\n",
    "print(\"The accuracy of the classifier on the test set is %.3f\" % accuracy)\n",
    "\n",
    "# Predict the sentiment of a negative review\n",
    "review = \"The movie was terrible. The music was underwhelming and the acting mediocre.\"\n",
    "prediction = clf.predict(vectorizer.transform([review]))[0]\n",
    "print(\"The sentiment predicted by the classifier is %i\" % (prediction))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    The accuracy of the classifier on the test set is 0.732\n",
    "    The sentiment predicted by the classifier is 0\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2626b3c3",
   "metadata": {},
   "source": [
    "## Building n-gram models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41751a6",
   "metadata": {},
   "source": [
    "### n-gram models for movie tag lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bcba46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate n-grams upto n=1\n",
    "vectorizer_ng1 = CountVectorizer(ngram_range=(1,1))\n",
    "ng1 = vectorizer_ng1.fit_transform(corpus)\n",
    "\n",
    "# Generate n-grams upto n=2\n",
    "vectorizer_ng2 = CountVectorizer(ngram_range=(1,2))\n",
    "ng2 = vectorizer_ng2.fit_transform(corpus)\n",
    "\n",
    "# Generate n-grams upto n=3\n",
    "vectorizer_ng3 = CountVectorizer(ngram_range=(1,3))\n",
    "ng3 = vectorizer_ng3.fit_transform(corpus)\n",
    "\n",
    "# Print the number of features for each model\n",
    "print(\"ng1, ng2 and ng3 have %i, %i and %i features respectively\" % (ng1.shape[1], ng2.shape[1], ng3.shape[1]))\n",
    "\n",
    "\"\"\"\n",
    "    ng1, ng2 and ng3 have 6614, 37100 and 76881 features respectively\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf10bef",
   "metadata": {},
   "source": [
    "### Higher order n-grams for sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bffc6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an instance of MultinomialNB \n",
    "clf_ng = MultinomialNB()\n",
    "\n",
    "# Fit the classifier\n",
    "clf_ng.fit(X_train_ng, y_train)\n",
    "\n",
    "# Measure the accuracy\n",
    "accuracy = clf_ng.score(X_test_ng, y_test)\n",
    "print(\"The accuracy of the classifier on the test set is %.3f\" % accuracy)\n",
    "\n",
    "# Predict the sentiment of a negative review\n",
    "review = \"The movie was not good. The plot had several holes and the acting lacked panache.\"\n",
    "prediction = clf_ng.predict(ng_vectorizer.transform([review]))[0]\n",
    "print(\"The sentiment predicted by the classifier is %i\" % (prediction))\n",
    "\n",
    "\"\"\"\n",
    "    The accuracy of the classifier on the test set is 0.758\n",
    "    The sentiment predicted by the classifier is 0\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94767d42",
   "metadata": {},
   "source": [
    "### Comparing performance of n-gram models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aae89a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "# Splitting the data into training and test sets\n",
    "train_X, test_X, train_y, test_y = train_test_split(df['review'], df['sentiment'], test_size=0.5, random_state=42, stratify=df['sentiment'])\n",
    "\n",
    "# Generating ngrams\n",
    "vectorizer = CountVectorizer()\n",
    "train_X = vectorizer.fit_transform(train_X)\n",
    "test_X = vectorizer.transform(test_X)\n",
    "\n",
    "# Fit classifier\n",
    "clf = MultinomialNB()\n",
    "clf.fit(train_X, train_y)\n",
    "\n",
    "# Print accuracy, time and number of dimensions\n",
    "print(\"The program took %.3f seconds to complete. The accuracy on the test set is %.2f. \n",
    "The ngram representation had %i features.\" % (time.time() - start_time, clf.score(test_X, test_y), train_X.shape[1]))\n",
    "      \n",
    "\"\"\"\n",
    "The program took 0.141 seconds to complete. The accuracy on the test set is 0.75. The ngram representation had 12347 features.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f826a0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "# Splitting the data into training and test sets\n",
    "train_X, test_X, train_y, test_y = train_test_split(df['review'], df['sentiment'], test_size=0.5, random_state=42, stratify=df['sentiment'])\n",
    "\n",
    "# Generating ngrams\n",
    "vectorizer = CountVectorizer(ngram_range=(1,3))\n",
    "train_X = vectorizer.fit_transform(train_X)\n",
    "test_X = vectorizer.transform(test_X)\n",
    "\n",
    "# Fit classifier\n",
    "clf = MultinomialNB()\n",
    "clf.fit(train_X, train_y)\n",
    "\n",
    "# Print accuracy, time and number of dimensions\n",
    "print(\"The program took %.3f seconds to complete. The accuracy on the test set is %.2f. \n",
    "The ngram representation had %i features.\" % (time.time() - start_time, clf.score(test_X, test_y), train_X.shape[1]))\n",
    "      \n",
    "\"\"\"\n",
    "The program took 0.971 seconds to complete. The accuracy on the test set is 0.77. The ngram representation had 178240 features.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1339a9",
   "metadata": {},
   "source": [
    "# 4. TF-IDF and similarity scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc45921e",
   "metadata": {},
   "source": [
    "## Building tf-idf document vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9699a68",
   "metadata": {},
   "source": [
    "### tf-idf vectors for TED talks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4b45f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create TfidfVectorizer object\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Generate matrix of word vectors\n",
    "tfidf_matrix = vectorizer.fit_transform(ted)\n",
    "\n",
    "# Print the shape of tfidf_matrix\n",
    "print(tfidf_matrix.shape)\n",
    "\n",
    "#  (500, 29158)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1adcea66",
   "metadata": {},
   "source": [
    "## Cosine similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d57f7f",
   "metadata": {},
   "source": [
    "### Computing dot product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834afd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize numpy vectors\n",
    "A = np.array([1,3])\n",
    "B = np.array([-2, 2])\n",
    "\n",
    "# Compute dot product\n",
    "dot_prod = np.dot(A, B)\n",
    "\n",
    "# Print dot product\n",
    "print(dot_prod)\n",
    "\n",
    "# 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395a5009",
   "metadata": {},
   "source": [
    "### Cosine similarity matrix of a corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a039b63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an instance of tf-idf Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Generate the tf-idf vectors for the corpus\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Compute and print the cosine similarity matrix\n",
    "cosine_sim = cosine_similarity(tfidf_matrix,tfidf_matrix)\n",
    "print(cosine_sim)\n",
    "\n",
    "\"\"\"\n",
    "corpus:\n",
    " ['The sun is the largest celestial body in the solar system', 'The solar system consists of the sun and eight revolving planets', 'Ra was the Egyptian Sun God', 'The Pyramids were the pinnacle of Egyptian architecture', 'The quick brown fox jumps over the lazy dog']\n",
    "\n",
    "<script.py> output:\n",
    "    [[1.         0.36413198 0.18314713 0.18435251 0.16336438]\n",
    "     [0.36413198 1.         0.15054075 0.21704584 0.11203887]\n",
    "     [0.18314713 0.15054075 1.         0.21318602 0.07763512]\n",
    "     [0.18435251 0.21704584 0.21318602 1.         0.12960089]\n",
    "     [0.16336438 0.11203887 0.07763512 0.12960089 1.        ]]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64864d43",
   "metadata": {},
   "source": [
    "## Building a plot line based recommender"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ddf960",
   "metadata": {},
   "source": [
    "### Comparing linear_kernel and cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b153463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record start time\n",
    "start = time.time()\n",
    "\n",
    "# Compute cosine similarity matrix\n",
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# Print cosine similarity matrix\n",
    "print(cosine_sim)\n",
    "\n",
    "# Print time taken\n",
    "print(\"Time taken: %s seconds\" %(time.time() - start))\n",
    "\n",
    "\"\"\"\n",
    "    [[1.         0.         0.         ... 0.         0.         0.        ]\n",
    "     [0.         1.         0.         ... 0.         0.         0.        ]\n",
    "     [0.         0.         1.         ... 0.         0.01418221 0.        ]\n",
    "     ...\n",
    "     [0.         0.         0.         ... 1.         0.01589009 0.        ]\n",
    "     [0.         0.         0.01418221 ... 0.01589009 1.         0.        ]\n",
    "     [0.         0.         0.         ... 0.         0.         1.        ]]\n",
    "    Time taken: 1.0700690746307373 seconds\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0926094b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record start time\n",
    "start = time.time()\n",
    "\n",
    "# Compute cosine similarity matrix\n",
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# Print cosine similarity matrix\n",
    "print(cosine_sim)\n",
    "\n",
    "# Print time taken\n",
    "print(\"Time taken: %s seconds\" %(time.time() - start))\n",
    "\n",
    "\"\"\"\n",
    "    [[1.         0.         0.         ... 0.         0.         0.        ]\n",
    "     [0.         1.         0.         ... 0.         0.         0.        ]\n",
    "     [0.         0.         1.         ... 0.         0.01418221 0.        ]\n",
    "     ...\n",
    "     [0.         0.         0.         ... 1.         0.01589009 0.        ]\n",
    "     [0.         0.         0.01418221 ... 0.01589009 1.         0.        ]\n",
    "     [0.         0.         0.         ... 0.         0.         1.        ]]\n",
    "    Time taken: 1.0700690746307373 seconds\n",
    "\n",
    "<script.py> output:\n",
    "    [[1.         0.         0.         ... 0.         0.         0.        ]\n",
    "     [0.         1.         0.         ... 0.         0.         0.        ]\n",
    "     [0.         0.         1.         ... 0.         0.01418221 0.        ]\n",
    "     ...\n",
    "     [0.         0.         0.         ... 1.         0.01589009 0.        ]\n",
    "     [0.         0.         0.01418221 ... 0.01589009 1.         0.        ]\n",
    "     [0.         0.         0.         ... 0.         0.         1.        ]]\n",
    "    Time taken: 0.24660348892211914 seconds\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace1acfd",
   "metadata": {},
   "source": [
    "### Plot recommendation engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed80a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the TfidfVectorizer \n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "# Construct the TF-IDF matrix\n",
    "tfidf_matrix = tfidf.fit_transform(movie_plots)\n",
    "\n",
    "# Generate the cosine similarity matrix\n",
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    " \n",
    "# Generate recommendations\n",
    "print(get_recommendations('The Dark Knight Rises', cosine_sim, indices))\n",
    "\n",
    "\"\"\"\n",
    "    1                              Batman Forever\n",
    "    2                                      Batman\n",
    "    3                              Batman Returns\n",
    "    8                  Batman: Under the Red Hood\n",
    "    9                            Batman: Year One\n",
    "    10    Batman: The Dark Knight Returns, Part 1\n",
    "    11    Batman: The Dark Knight Returns, Part 2\n",
    "    5                Batman: Mask of the Phantasm\n",
    "    7                               Batman Begins\n",
    "    4                              Batman & Robin\n",
    "    Name: title, dtype: object\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8fddc4",
   "metadata": {},
   "source": [
    "### The recommender function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c51e966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate mapping between titles and index\n",
    "indices = pd.Series(metadata.index, index=metadata['title']).drop_duplicates()\n",
    "\n",
    "def get_recommendations(title, cosine_sim, indices):\n",
    "    # Get index of movie that matches title\n",
    "    idx = indices[title]\n",
    "    # Sort the movies based on the similarity scores\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    # Get the scores for 10 most similar movies\n",
    "    sim_scores = sim_scores[1:11]\n",
    "    # Get the movie indices\n",
    "    movie_indices = [i[0] for i in sim_scores]\n",
    "    # Return the top 10 most similar movies\n",
    "    return metadata['title'].iloc[movie_indices]\n",
    "\n",
    "\"\"\"\n",
    "               title                                            tagline\n",
    "938  Cinema Paradiso  A celebration of youth, friendship, and the ev...\n",
    "630         Spy Hard  All the action. All the women. Half the intell...\n",
    "682        Stonewall                    The fight for the right to love\n",
    "514           Killer                    You only hurt the one you love.\n",
    "365    Jason's Lyric                                   Love is courage.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29481714",
   "metadata": {},
   "source": [
    "### TED talk recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d810d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the TfidfVectorizer \n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "# Construct the TF-IDF matrix\n",
    "tfidf_matrix = tfidf.fit_transform(transcripts)\n",
    "\n",
    "# Generate the cosine similarity matrix\n",
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    " \n",
    "# Generate recommendations\n",
    "print(get_recommendations('5 ways to kill your dreams', cosine_sim, indices))\n",
    "\n",
    "\"\"\"\n",
    "    453             Success is a continuous journey\n",
    "    157                        Why we do what we do\n",
    "    494                   How to find work you love\n",
    "    149          My journey into movies that matter\n",
    "    447                        One Laptop per Child\n",
    "    230             How to get your ideas to spread\n",
    "    497         Plug into your hard-wired happiness\n",
    "    495    Why you will fail to have a great career\n",
    "    179             Be suspicious of simple stories\n",
    "    53                          To upgrade is human\n",
    "    Name: title, dtype: object\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b7f138",
   "metadata": {},
   "source": [
    "## Beyond n-grams: word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f2ae42",
   "metadata": {},
   "source": [
    "### Generating word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b8e107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the doc object\n",
    "doc = nlp(sent)\n",
    "\n",
    "# Compute pairwise similarity scores\n",
    "for token1 in doc:\n",
    "  for token2 in doc:\n",
    "    print(token1.text, token2.text, token1.similarity(token2))\n",
    "    \n",
    "\"\"\"\n",
    "I like apples and oranges\n",
    "\n",
    "<script.py> output:\n",
    "    I I 1.0\n",
    "    I like 0.13463897\n",
    "    I apples -0.036133606\n",
    "    I and -0.085230574\n",
    "    I oranges 0.033708632\n",
    "    like I 0.13463897\n",
    "    like like 1.0\n",
    "    like apples 0.0007651703\n",
    "    like and 0.104521796\n",
    "    like oranges -0.045859136\n",
    "    apples I -0.036133606\n",
    "    apples like 0.0007651703\n",
    "    apples apples 1.0\n",
    "    apples and -0.051072996\n",
    "    apples oranges 0.46452007\n",
    "    and I -0.085230574\n",
    "    and like 0.104521796\n",
    "    and apples -0.051072996\n",
    "    and and 1.0\n",
    "    and oranges 0.038236685\n",
    "    oranges I 0.033708632\n",
    "    oranges like -0.045859136\n",
    "    oranges apples 0.46452007\n",
    "    oranges and 0.038236685\n",
    "    oranges oranges 1.0\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b14a25",
   "metadata": {},
   "source": [
    "### Computing similarity of Pink Floyd songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510a891c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Doc objects\n",
    "mother_doc = nlp(mother)\n",
    "hopes_doc = nlp(hopes)\n",
    "hey_doc = nlp(hey)\n",
    "\n",
    "# Print similarity between mother and hopes\n",
    "print(mother_doc.similarity(hopes_doc))\n",
    "\n",
    "# Print similarity between mother and hey\n",
    "print(mother_doc.similarity(hey_doc))\n",
    "\n",
    "\"\"\"\n",
    "<script.py> output:\n",
    "    0.39086030814019257\n",
    "    0.8043759483951038\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
